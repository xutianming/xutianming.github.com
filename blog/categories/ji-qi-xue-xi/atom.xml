<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 机器学习 | Xutianming Blog]]></title>
  <link href="http://xutianming.github.io/blog/categories/ji-qi-xue-xi/atom.xml" rel="self"/>
  <link href="http://xutianming.github.io/"/>
  <updated>2013-09-22T13:45:35+08:00</updated>
  <id>http://xutianming.github.io/</id>
  <author>
    <name><![CDATA[Xutianming]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[卡方校验、信息增益和互信息]]></title>
    <link href="http://xutianming.github.io/blog/2013/09/14/chi-square-information-gain-and-mutual-information/"/>
    <updated>2013-09-14T20:51:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/09/14/chi-square-information-gain-and-mutual-information</id>
    <content type="html"><![CDATA[<p>从一开始做文本分类到现在做的项目，一直在不断的学习新的知识，练习已经掌握的工具，也越来越深刻的认识到，“你所掌握的机器学习方法，并不是死板的一成不变的算法，而是一个一个的小工具，是解决问题的思路”。</p>

<p>这篇博客将对比三个在我进行文本挖掘相关工作的时候经常用到的小工具，以及我对他们的认识，仅限个人的粗浅观点，也是我当前的认识，不见得正确。</p>

<p><a href="http://zh.wikipedia.org/wiki/%E7%9A%AE%E7%88%BE%E6%A3%AE%E5%8D%A1%E6%96%B9%E6%AA%A2%E5%AE%9A">卡方校验</a></p>

<p>首先理解卡方校验的概念，除了维基百科，我总会看<a href="http://www.blogjava.net/zhenandaci/archive/2008/08/31/225966.html">这篇博客</a>。简单来讲，卡方校验是用来判别两个事件是否相关的方法，因此在特征提取、关键词提取中都有很多的应用。基本思路是，首先假设A、B不相关，然后根据A和B的联合概率分布，计算卡方值，公式在定义中都有给出，卡方值越大，越倾向于推翻原假设，也就是A、B越相关。在计算卡方值的过程中，最关键的问题是找到联合概率分布。那么我们应该如何理解卡方值呢？卡方值的含义其实是这样的，我们首先假设A、B不相关，也就是AB相互独立，那么我们可以为联合概率分布中的AB同时成立的事件计算期望值，根据定义公式显示，卡方值是这个期望值和实际值之间的差的平方除以期望值的和（定义和方差很像！），衡量了这两个量拟合的程度。那么，卡方值越大，代表期望值和实际值之间的差越大，也就是我们计算期望值所依赖的假设，是不成立的！</p>

<p>当我们想用卡方检验的方法，去判断两个事件是否相关的时候，理解问题的过程，就是建立事件A、B联合概率分布的过程，找到这个概率分布，就可以计算卡方值。举例说明的话，在文本分类的特征提取这一步，事件A是特征词w是否存在在本篇文档中，事件B是本篇文档是否属于类c。首先假设“w不是类c的特征词”，那么词w在属于类c的所有文档中出现的概率，应当与w在全部文档中出现的概率相同，我们依据这一假设，可以算出w出现在c类的文档数的期望，用期望值和实际值的差的平方除以作为卡方值。</p>

<p><a href="http://zh.wikipedia.org/wiki/%E7%9B%B8%E5%AF%B9%E7%86%B5">信息增益</a></p>

<p>也经常作为特征提取的方法，也用来衡量两个事件的相关性，但是是从信息论的角度来的。信息论用熵来衡量信息量。信息增益，简单理解就是在已知条件B的情况下，事件A的信息量的改变程度，就是熵的改变程度。同样有<a href="http://www.blogjava.net/zhenandaci/archive/2009/03/24/261701.html">一篇博客</a>讲得很透彻。从上面的描述可以看出，计算信息增益的关键是计算熵和条件熵。所以说，信息增益对应的概率分布，是普通的概率分布和一个条件概率分布。那么我们应该如何具体理解信息增益呢？我们首先计算事件A的信息熵，然后考虑事件B，计算在有事件B的情况下的条件熵，两者之差，就是信息熵的改变，改变越大，说明事件B对于事件A的相关性越大。由于涉及到信息论，所以信息量改变这一点，就变的略显抽象。</p>

<p>仿照上文中对卡方校验的介绍。利用信息增益解决问题的关键，在于找到这个条件概率分布，以计算条件熵。同样举文本分类中特征提取这个例子。事件A是指文档属于类ci，事件B是词w是否出现在文档中。根据P(Ci|w)和P(Ci|^w)这两个条件概率分布，可以计算条件熵H(C|T)。</p>

<p>不过仔细读过信息增益的概念，发现我们上面介绍的和KL散度差别挺大的，目前我也不能理解这两种概念是否本质上相同，但是在分类和挖掘领域，大家一般是像我说的那样去理解信息增益的。</p>

<p><a href="http://zh.wikipedia.org/wiki/%E4%BA%92%E4%BF%A1%E6%81%AF">互信息</a></p>

<p>在互信息定义的多条公式中，我一般使用第三个，就是X，Y熵之和减去联合熵。根据联合熵，就可以发现，决定互信息的，也是一个联合概率分布。</p>

<p>比较形象的理解互信息的公式方法是，认为事件X和事件Y相关，可以让总信息熵减少多少，差别越大，X和Y越相关。计算互信息的关键，是计算联合熵，因此在用互信息解决问题的适合，找到这个联合概率分布，是核心。不要试图去寻找其他的概率分布，实践表明，互信息的联合概率分布与卡方检验的非常相似，都是x是否怎样，y是否怎样。同样以文本分类为例，X为文档是否属于类c，Y事件为文档是否包含w，然后利用这个概率分布去计算联合熵。这么一看，和卡方检验几乎完全相同！只是利用了不同的理论基础。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[核函数理解]]></title>
    <link href="http://xutianming.github.io/blog/2013/07/20/understanding-kernel-function/"/>
    <updated>2013-07-20T11:13:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/07/20/understanding-kernel-function</id>
    <content type="html"><![CDATA[<p>SVM相关的理论知识看过几遍，但是感觉一直都不是理解的很透彻，特别是核函数的概念。今天又查阅了大家写的学习笔记等资料，结合自己的实践经验，感觉对核函数的理解更进一步了，所以记录在这里。</p>

<p>一句话说明核函数的意义和作用：为使非线性可分问题转化为线性可分，将特征向量映射到高维，映射函数所满足的性质。</p>

<p>具体说来，svm可以直接解决线性可分的分类问题，对于近似线性可分的问题，通过增加松弛变量，也可以实现分类。那么对于非线性可分的问题呢，比较直观的想法是映射，也就是通过把特征映射到高维，使非线性可分问题转化为线性可分的问题。但是就一般的实际问题来讲，寻找这个映射很困难，而且很难有可推广的方法。于是提出了kernel trick。可以绕过寻找这个映射函数。</p>

<p>在理论公式的推导中，可以算出参数w的表示方法：</p>

<p><img src="http://i.imgur.com/KqPVcLl.png?1" alt="Imgur" /></p>

<p>于是分类超平面的函数也可以写成：</p>

<p><img src="http://i.imgur.com/UVGLjMS.png?3" alt="Imgur" /></p>

<p>这个公式中只有&lt;x(i),x>的内积形式，也就是说，我们映射之后，需要计算&lt;Fi(xi),Fi(x)>的内积。而我们定义的核函数，就是这一内积的形式。</p>

<p><img src="http://i.imgur.com/lLicBOp.png?1" alt="Imgur" /></p>

<p>如此一来，我们就可以用核函数直接替换内积，相当于，我们把原始的特征向量，映射到了高维，并且避免了寻找映射函数。而libsvm中推荐使用的RBF核函数，就是一个有着良好特性的核函数。可以将样本映射到一个更高维的空间，可以处理非线性的分类问题。而其关键的参数c和g分别为惩罚因子和核函数的参数。（RBF 核K(x, y) = exp(－γ || x －y ||的平方),γ > 0，g为gamma，就是r）。我们在使用libsvm训练之前，应该首先寻找最优的参数c和γ。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[文本分类实践]]></title>
    <link href="http://xutianming.github.io/blog/2013/07/19/text-classification-in-action/"/>
    <updated>2013-07-19T22:14:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/07/19/text-classification-in-action</id>
    <content type="html"><![CDATA[<p>学习机器学习有一阵子了，先是仔细阅读了《统计学习方法》这本书，对目前比较流行的统计学习方法和模型有了大致的认识，比如朴素贝叶斯、逻辑斯蒂回归、支持向量机、隐马尔科夫模型等等。但是仅仅有理论上的认识是不够的，关键还是指导实践解决问题。</p>

<p>先前在百度实习的时候，已经对机器学习有了初步的认识，并且当时也利用逻辑斯蒂回归训练了一个还算不错的模型，虽然最终由于时间原因，没有做成一个可以应用到线上的模块，但是对于我对机器学习的这第一次尝试，我还是很满意的，当时也决定在研二的这一年里，好好研究一下机器学习相关的东西，以后好找相关方向的工作。</p>

<p>最初的实践决定从文本分类开始，因为这个领域相对来说研究的比较成熟，并且在工业界仍然有很多很重要的应用。由于svm一直是个非常重要的机器学习模型，之前对原理有一定的了解，于是决定从使用svm开始。网上相关的教程很多，我也是慢慢参考着别人的方法一步一步尝试。这里主要记录下我实践的过程，以供日后参考。</p>

<p>文本分类大致分为以下几步：</p>

<ul>
<li>数据预处理（一般是分词，去停）</li>
<li>特征选择（比较好的方法有卡方校验和信息增益）</li>
<li>特征值确定（比较好的是tf-idf值）</li>
<li>特征向量计算</li>
<li>模型训练</li>
<li>模型测试</li>
</ul>


<p>我使用的是复旦大学自然语言理解实验室提供的预料，选取了computer、agriculture、sports和history，选择分类的时候特意选择了差别比较大的几类，这样可以在某种程度上提高分类效果吧。</p>

<p>首先是数据预处理，我使用了同事推荐了一个分词工具，分词的效果不是很好，算是个初级的分词工具，基于python的，使用起来非常简单。分词之后，把预料中的文章，处理成了一个词一行的文档，便于下一步的处理。</p>

<p>数据预处理之后，就是选择特征了，这里所谓的“特征”，就是上一步分出来的一个一个的词，每个词都可以说是一个特征，但是把每个特征都哪来使用，显然是不现实的，太高维的特征向量，首先训练起来比较费时，另外会使模型复杂度提升从而容易使svm训练过拟合。我训练的时候，本想使用1w维的特征，最终使用了7600左右的特征，具体的我后面会说。考虑到以上原因，所以要有个特征选择的过程，也就是选择那些可以很好的区分每一类的词，比如包含了“农业”一词的文章，属于农业类的可能性比较大，所以这样的词，就算是好特征。那么如何筛选出这些特征呢，要有个数学上的量，来衡量特征的好坏，可供选择的有互信息、信息增益、卡方校验等方法。其中互信息的效果比较差，我就没自己尝试，我先后使用了信息增益和卡方校验的方法。</p>

<p>卡方校验，学名是“皮尔森卡方检验”，是一种假设检验方法。首先，我们的假设是“词word和类class不相关”，然后针对包含word的文档和以及属于class的文档的联合分布，算卡方值，每类选择卡方值最大的2500个值，去作为特征值，因为卡方值越大，倾向于原假设为假，也就是说，卡方值越大，词word和类class越相关，这个词就是我们要找的好特征。为啥是2500个值，因为我初步决定选择1w个特征么，一共四类，每一类选择2500个。最终计算下来，因为分词效果的原因，很多单字的特征被筛选出来，但是单字的信息非常少，我觉得不足以作为一个好的特征，所以我对于筛选出来的1w个特征，去除了那些单字的词，就剩下了大约7600个词作为特征。</p>

<p>特征选择出来之后，我们要利用这些特征去表示要分类的文档，也就是所谓的“把文档映射到特征空间”，用向量来表示文档。这里会遇到一个问题，如何用特征表示文档，另外每个特征对于一个文档都是等同重要的嘛？这里就自然涉及到为特征赋权重。我是用的是tf-idf值。选择这个的原因是，卡方校验有个缺陷，就是“低频率缺陷”，那些在一篇文档里只出现过一次的词，也有可能会有很大的卡方值，但是这个词，对这篇文章可能没太大价值，那些只在某一类文章中大量出现，而在其他类中很少出现的词，才是决定类别的词。而我个人认为tf-idf值可以比较好的弥补这一缺陷。当然我只是定性分析，我不太懂数学上的证明，也没去查阅相关的资料。</p>

<p>如此一来，我们把文档用特征向量来表示了，接下来就是训练模型了。在上述过程的实现过程中，我没有遇到太大的问题，unix+python是很有利的文本处理工具，实现起来很快。我是用的svm工具是libsvm，使用也很简单，但是我确实在训练模型的时候遇到了一个问题。</p>

<p>一般来讲，libsvm的使用流程是这样的：</p>

<ul>
<li>svm-scale，对数据进行归一化处理，映射到[0,1]区间内</li>
<li>grid.py 寻找最优参数c和g</li>
<li>svm-train</li>
<li>svm-predict</li>
</ul>


<p>我第一次使用的时候，用信息增益选择特征，libsvm不太懂，也没去查太多的资料，略去了第二步，也就是我训练的时候没指定参数。这训练效果太差了，把所有的文档都分到了一类中。一开始我总怀疑特征提取的不好，于是换卡方校验，结果还是这样，之后开始怀疑我训练的方法不对，于是很快发现了问题。在指定c和g参数之后，训练结果非常好，对训练数据的准确率为100%，在测试集上的准确率为93.52%，比人家论文上的准确率还高，非常让人振奋。</p>

<p>至此，算是完满的完成了一次svm文本分类的实践，结果也符合之前给自己定的目标90%以上的准确率。接下来，我会根据这次实践的结果，总结分析，再回归到svm的理论上，比如c和g具体含义对应的是什么，借此机会深入理解svm，好更好的掌握这个工具的使用。然后还会再用同样的方法，实践体会其他的分类模型，比如逻辑斯蒂回归、神经网络等等。</p>
]]></content>
  </entry>
  
</feed>
