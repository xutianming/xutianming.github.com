<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 机器学习 | Xutianming Blog]]></title>
  <link href="http://xutianming.github.io/blog/categories/ji-qi-xue-xi/atom.xml" rel="self"/>
  <link href="http://xutianming.github.io/"/>
  <updated>2013-11-14T11:35:03+08:00</updated>
  <id>http://xutianming.github.io/</id>
  <author>
    <name><![CDATA[Xutianming]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[行业关键词分类项目总结1]]></title>
    <link href="http://xutianming.github.io/blog/2013/11/14/text-classification-step-by-step/"/>
    <updated>2013-11-14T10:24:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/11/14/text-classification-step-by-step</id>
    <content type="html"><![CDATA[<p>在过去的一个月里，完成了一个行业关键词分类的项目，100w的训练数据，1000w的测试数据，33个分类。项目期限已经到了，但是我们的准确率目前只有88%，尝试了很多方法，遇到很多困难，也走过弯路。</p>

<p>项目结束并且效果不好，自然要寻找原因。一方面是等待比赛完全结束，公布冠军的解决方法，来开阔下眼界；另一方面就是更广泛的读一些相关的资料，做一些在项目过程中没有精力去做的阅读和学习工作。</p>

<p>首先简单描述下项目的过程。这次项目提供了33个分类，和一些要分类的关键词。每个关键词，提供了10个搜索引擎的自然排序的结果，以及购买这些关键词的用户。一部分已经标注的训练数据，和大量要分类的测试数据，分类效果的评估用的是宏平均(Macro-average)准确率。接下来，我们做了如下尝试：</p>

<ol>
<li>利用提供的文本数据进行文本分类。其中包括了利用lucene的IKAnalyzer进行分词、利用Chi-square进行特征选择、利用tf-idf作为权重生成文本的向量表示形式、利用Liblinear进行线性核的分类。第一次准确率87%。由于以上工作都是在Hadoop上完成的，加上学习和适应Hadoop平台的时间，其实只要一天的工作量的事情，做了2周（比赛承办方提供的Hadoop集群的限制实在是太多了）。</li>
<li>利用一些人工制定的规则，挖掘一些有着高区分度的词，利用这些词去修正上述文本分类的结果。这一步，把准确率提高到了88.5%。这也是我们最好的成绩了，本来可以利用这种方法获得更多的准确率的提升的，但是感觉可学习的东西很多，不想把更多的时间放在对数据的理解上。</li>
<li>利用关键词的用户购买信息去做关键词分类。这样做是建立在“购买关键词的用户相似，那么关键词倾向于属于同一类”这一假设的基础上的。所以想对&lt;关键词-用户>矩阵，应用分类算法。这一矩阵存在着维度过高（600w维）和过于稀疏的问题（很少有两个用户买同一个关键字）。所以推断直接分类既不可行，效果也不好。</li>
<li>这一条详细讲一下用户矩阵的使用。首先想到的是对用户聚类，使用k-means。但是由于上面所说的稀疏性问题，所以聚类效果也不好。于是想到利用第一次文本分类的结果，补充训练数据，另外首先对用户聚类（已经聚好了，33类），这样&lt;用户-关键词>矩阵维度33维，并且不稀疏。可以使用k-means对用户进行聚类了，分别制定k为100和1000，进行聚类。在使用的过程中，发现了一个问题，k-means一般是基于距离的，而在这个项目的用户中，和质心距离相等的两个点，并不一定相同，距离计算过程中把所有的维度（关键词类别）等同看待，而现实中，我们购买了不同维度（类别）的关键词的用户，虽然距离相等，但是是属于不同类的。基于上述思考，我放弃了使用k-means，按照自己的想法，对用户进行了聚类。得到了2w类左右的用户。用户聚类之后，就达到了对&lt;关键词-用户>矩阵的降维和去稀疏的目的。可以应用分类算法了。</li>
<li>利用上述的方法，利用用户对关键词分类。但是结果非常不理想。原因在于，我们在第3步里的假设是不对的。也就是说，假如几个关键词，被几个相同或者相似的用户购买，并不代表关键词属于同一类。分析bad case可以看出，比如公司转让，服装公司转让被分入了服装类，房地产公司转让被分入了房地产类。但是这些关键词应该是被一个公司转让中介性质的用户购买了。所以从用户角度去分类，是不可行的。</li>
<li>经过上面的尝试，时间已经过去了3周。我们放弃了使用用户购买信息，试图提高文本分类的效果。首先我们发现，需要利用多词模式去提高特征质量，比如，搬家是个特征词，公司不是，但是搬家公司，是个非常有区分度的特征。由于没有事先的积累，也找不到现成的工具，我们自己根据经验和对数据的理解，写了bi-gram和tri-gram的map-reduce组合算法。使用频率对多词模式进行筛选，然后加入特征集训练。这一步，问题很多，因为大部分是拍脑袋定的，感觉很不科学，接下来，我可能会对这方面做一些针对性的阅读。</li>
<li>重复了文本分类的步骤，做了第二版的分类器，但是由于多词模式的筛选和使用都有些问题，所以这一版分类器没有取得预期的进步。</li>
<li>时间到了最后一周，感觉也没有什么特别好的点子了。偶然之中发现了Multi-class 分类相关的知识，比如one-vs-rest，one-vs-one等等。以为发现了新的方法，于是我们用Hadoop又做了一个one-vs-rest的多类分布式分类器，主要的思路是参考的别人的，花了很多时间去读别人的代码以及了解liblinear的各项接口和参数。这段时间算是对SVM和Hadoop编程有了更进一步的知识。无奈，最终发现，其实本来Liblinear做多类分类，就是用的one-vs-rest。而且我们的分类器，最终效果也不好。</li>
</ol>


<p>这就是过去一个月中，我们进行的所有工作。学到了很多知识，对SVM，NB分类器，对Hadoop的java接口和Streaming以及map-reduce的思想都有了进一步的认识，并且可以顺利使用了。虽然最终结果并不好，但是对于缺乏经验的我们来说，算是一个很好的经历。下篇文章里，我再结合一些书本上的知识，讲一些关于这个项目的思考，和可能的改进方向。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MapReduce初试]]></title>
    <link href="http://xutianming.github.io/blog/2013/10/29/mapreduce-program-trial/"/>
    <updated>2013-10-29T20:46:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/10/29/mapreduce-program-trial</id>
    <content type="html"><![CDATA[<p>最近在尝试做一个行业关键词分类的项目，训练数据百万级，测试数据千万。数据量不大，但是我的机器跑来已经相当吃力了，于是想用一些简单的Hadoop知识，体验一下这个目前业内最流行的分布式计算框架。</p>

<p>以前接触过很多分布式计算的知识，也学习过MapReduce。甚至实习期间，还多少使用到了hadoop的hive。但是一直都流于表面和概念，没有机会实践，甚至都不知道一个MapReduce程序如何来写，这次终于有机会尝试。</p>

<p>体验的过程并不顺利，为了简单起见，我使用Python和Streaming。参考网上的博客，很快就写出了MapReduce程序。这里的心得是，编写MapReduce程序并不比普通的编程难，关键是把算法分布式化，思考清楚，在Map和Reduce的过程中的数据流。这里分别附上我写的简单的Map，Reduce程序，作为以后工作的模板。</p>

<p>Mapper.py</p>

<pre><code>#!/usr/bin/env python
import math
import sys

features = []

fin = open("feature.txt","r")
for line in fin:
    winfo = line.split("\t")
    word = winfo[0].strip()
    features.append(word)
fin.close()


for line in sys.stdin :
    document = []
    info = line.strip().split("\t")
    for i in range(2,len(info)):
        document.append(info[i].strip())
    for feature in features:
        if feature in document :
            print '%s\t%s' % (feature, 1)
</code></pre>

<p>Reducer.py</p>

<pre><code>#!/usr/bin/env python
import math
import sys

current_word = None
current_count = 0
word = None

for line in sys.stdin:
    line = line.strip()
    word,count = line.split('\t')

try:
    count = int(count)
except ValueError:
    continue

if current_word == word:
    current_count += count
else :
    if current_word :
        print '%s\t%s' % (current_word,current_count)
    current_count = count
    current_word = word

if current_word == word:
    print '%s\t%s' % (current_word, current_count)
</code></pre>

<p>这一个统计DocumentFrequence的MapReduce程序。下面就一一描述下在编写这个程序中遇到的问题和解决方案。</p>

<ol>
<li>首先注意到Mapper中需要读入一个文件。一开始不懂文件应该怎么才可以被程序读到。尝试过放在HDFS和改变各种文件路径。其实只要在执行hadoop任务的时候，用<code>-file</code>参数指定就好了。</li>
<li>关于失败提示。<code># of failed Map Tasks exceeded allowed limit</code>,一开始的时候总是遇到。出现这个job失败提示的原因在于mapper程序有问题。执行失败，超过限度，job就会被杀死。所以mapper要做好本地测试。使用<code>cat input|python mapper.py|sort|reducer.py</code>可以对程序进行基本的测试。再就是在本地的hadoop平台上先跑一下，假如说没问题了，可以跑个5%，一般就可以提交到线上的集群了。</li>
<li>关于Streaming命令的各个参数。除了使用<code>-mapper</code>，<code>-reducer</code>之外。还可以利用<code>-D</code>选项，指定一些hadoop的运行参数，比如mapper和reducer的数量。这里有个比较重要的技巧。就是假如我们的执行逻辑中，只需要mapper不需要reducer。有几种办法，一个是不指定reducer，默认使用<code>IdentityReducer</code>，mapper的输入直接转化为reducer的输出；二是设置reducer数量为0，<code>-setReduceTasks=0</code>，有几个mapper即有几个输出文件。</li>
<li>最重要的一点，程序效率。上面给出的两个模板很高效。9GB的输入文件，十分钟即可在线上集群中完成。但是一开始并不是这样的。这个问题困扰了我整个周末。一开始，我想当然以为既然hadoop了，程序效率就不是短板了，我效率低，mapper分块小一点也很快完成，于是编程序的时候就很不在意效率。用list查找什么的，各种遍历循环，怎么好写怎么来。于是一开始我的程序测试好提交之后，job执行很慢，可以理解。令人崩溃的是，总是莫名其妙失败，我一度认为是线上集群不稳定弱爆了。后来在乐乐的提示下改进了程序的效率，list改用了hash，提高查找效率，去掉了几个循环。于是job很顺利也很迅速执行完了。事后分析，我觉得线上的集群可能做了很多的限制，我之前的写法，由于消耗太多的资源，由于本身优先级不高，就总是被delay，就会有莫名其妙的失败。所以mapreduce程序中，效率也是非常重要的，每一个task都高效执行，那么整个job都能很快完成，大大提高效率，节省时间啊！终于深刻认识到程序效率的重要性了。</li>
<li>最后一点，我还没有很好的解决。就是mapreduce程序的调试。hadoop实战这本书上有介绍debug的方法，我没怎么看懂，另外简单的程序，使用print大法打印一些调试信息和程序运行信息，很有利于快速定位bug。目前还不知道如何打印。如果在本地的hadoop的话，可以输出到<code>stderr</code>上，会最终输出到logs目录下的日志里。但是线上集群的话，就没有结点的访问权限，自然没法查看日志。所以我就想能否通过<code>Reporter</code>来打印一些消息。还没有尝试过，需要实践来验证。</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[知识可视化项目第一阶段总结2]]></title>
    <link href="http://xutianming.github.io/blog/2013/09/24/summary-of-wikivis-project-2/"/>
    <updated>2013-09-24T21:15:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/09/24/summary-of-wikivis-project-2</id>
    <content type="html"><![CDATA[<p>今天和乐乐讨论了发表了Wikipedia Miner思路的论文，感觉颇深，故在此记录一下。</p>

<p>今天其实并没有讨论太多论文的内容，主要是基于项目第一阶段的反思，所以感觉还是很有意义的。这样的话，可以一步一步的提高我们解决问题的能力。关于面对一个要解决的课题的时候，我们从什么角度去思考它的解决方案，以及在目前的解决方案遇到了问题，走入了死胡同，如何调整路线，以最终找到那条正确的道路。</p>

<p>第一个问题是关于思考问题的角度。很明显，在第一阶段的项目进展中，我们并不成功，没有找到一种泛化能力非常好的算法，来对维基百科的数据进行有效的挖掘。在今天读了论文之后，发现论文作者和我们对于问题的思考思路是完全不同的。具体来讲，论文的课题是要解决“给哪些词加链接，有助于读者理解当前的文章”，而我们的目的是“有哪些关键字，可以很好的代表这篇文章，我们要给出这些关键字对应的维基百科链接”（这里仅指我们项目中的关键词打分步骤）。从这个角度看，我们要解决的问题其实是一致的，但是我们分别采用了什么样的思路来思考问题的呢？首先讲论文中的思路，维基百科中现有的有链接的文章是标注好的语料，那么满足什么条件的词是重要的关键字，需要加链接呢？我可以利用机器学习的方法来训练一个模型。首先根据经验选择一些特征，再利用有标注的训练数据，选择合适的算法，这样就可以学习模型了。这个模型告诉我们，在一篇文章里，那些词是关键字。那么，我之前使用的思路是什么呢？在语料充足的情况下，我们有很多统计量，比如卡方、平均值、标准差、余弦距离等等。那么用哪个统计量去描述关键词的特征最好呢？于是自然而然的想到了关键词提取，根据前人论文中提出的方法实现了。</p>

<p>这是两个解决问题的思路。不能说两个思路哪个好哪个坏，因为没什么可比性。这两种方法，前者立足于学习筛选关键字的标准，而后者立足于，我用什么样的标准，可以筛选出关键字。但是实践证明，解决我这个项目的问题，前者更好（Wikipedia Miner关键词挖掘效果很好）。因为在我的方法中，有个致命的弱点，就是语料。我是从一篇文章中根据统计规律去提取关键字，每个主题，使用什么词、什么词关键都满足一定的统计规律。那么语料不足怎么办？争取在不改变统计规律的前提下扩充语料，这一点太难做到了。而论文中的方法呢，学习的是标注关键字的方法，假定不论什么主题，哪些关键字重要，都是满足一定的准则的。显然后者更加符合维基百科的特点，详见<a href="http://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking">维基百科规则</a></p>

<p>下面讨论第二个问题，其实是个知识面的问题。因为最终乐乐发现，在我们读的这篇论文描述前人工作的时候提到了关键字提取相关的技术，在一篇综述性的文献中。而论文的作者，正是在前人工作的基础上，提出了这种方法，在2008年。按理说过去这么久了，为什么我们在最初确定技术方案的时候，没有找到现在这个更好的解决方案呢？因为我们使用的谷歌学术中没有收录那篇文章，并且在最初我们对于这个领域比较陌生的时候，很难使用准确的关键字去在搜索引擎上搜索，所以也就比较难一下子就找到最好的解决方案。也是经过了一段时间的研究和摸索，机缘巧合下，我的导师给我推荐了Wikipedia Miner。我才发现我们的问题已经有了这么优秀的解决方案了。</p>

<p>仔细想想呢，这其实涉及到了领域知识的问题。用什么技术，解决什么问题，是要经过长期实践积累经验的。随着阅历的提升，知道的办法越多，就越有解决问题的能力。在这一点上，我的导师自然大大的超越了我们。不过如何更有效的利用各种学术资源，会议门户啊、学术搜索啊、学术数据库等等，回头我还是要好好向我的导师请教一下。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[卡方校验、信息增益和互信息]]></title>
    <link href="http://xutianming.github.io/blog/2013/09/14/chi-square-information-gain-and-mutual-information/"/>
    <updated>2013-09-14T20:51:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/09/14/chi-square-information-gain-and-mutual-information</id>
    <content type="html"><![CDATA[<p>从一开始做文本分类到现在做的项目，一直在不断的学习新的知识，练习已经掌握的工具，也越来越深刻的认识到，“你所掌握的机器学习方法，并不是死板的一成不变的算法，而是一个一个的小工具，是解决问题的思路”。</p>

<p>这篇博客将对比三个在我进行文本挖掘相关工作的时候经常用到的小工具，以及我对他们的认识，仅限个人的粗浅观点，也是我当前的认识，不见得正确。</p>

<p><a href="http://zh.wikipedia.org/wiki/%E7%9A%AE%E7%88%BE%E6%A3%AE%E5%8D%A1%E6%96%B9%E6%AA%A2%E5%AE%9A">卡方校验</a></p>

<p>首先理解卡方校验的概念，除了维基百科，我总会看<a href="http://www.blogjava.net/zhenandaci/archive/2008/08/31/225966.html">这篇博客</a>。简单来讲，卡方校验是用来判别两个事件是否相关的方法，因此在特征提取、关键词提取中都有很多的应用。基本思路是，首先假设A、B不相关，然后根据A和B的联合概率分布，计算卡方值，公式在定义中都有给出，卡方值越大，越倾向于推翻原假设，也就是A、B越相关。在计算卡方值的过程中，最关键的问题是找到联合概率分布。那么我们应该如何理解卡方值呢？卡方值的含义其实是这样的，我们首先假设A、B不相关，也就是AB相互独立，那么我们可以为联合概率分布中的AB同时成立的事件计算期望值，根据定义公式显示，卡方值是这个期望值和实际值之间的差的平方除以期望值的和（定义和方差很像！），衡量了这两个量拟合的程度。那么，卡方值越大，代表期望值和实际值之间的差越大，也就是我们计算期望值所依赖的假设，是不成立的！</p>

<p>当我们想用卡方检验的方法，去判断两个事件是否相关的时候，理解问题的过程，就是建立事件A、B联合概率分布的过程，找到这个概率分布，就可以计算卡方值。举例说明的话，在文本分类的特征提取这一步，事件A是特征词w是否存在在本篇文档中，事件B是本篇文档是否属于类c。首先假设“w不是类c的特征词”，那么词w在属于类c的所有文档中出现的概率，应当与w在全部文档中出现的概率相同，我们依据这一假设，可以算出w出现在c类的文档数的期望，用期望值和实际值的差的平方除以作为卡方值。</p>

<p><a href="http://zh.wikipedia.org/wiki/%E7%9B%B8%E5%AF%B9%E7%86%B5">信息增益</a></p>

<p>也经常作为特征提取的方法，也用来衡量两个事件的相关性，但是是从信息论的角度来的。信息论用熵来衡量信息量。信息增益，简单理解就是在已知条件B的情况下，事件A的信息量的改变程度，就是熵的改变程度。同样有<a href="http://www.blogjava.net/zhenandaci/archive/2009/03/24/261701.html">一篇博客</a>讲得很透彻。从上面的描述可以看出，计算信息增益的关键是计算熵和条件熵。所以说，信息增益对应的概率分布，是普通的概率分布和一个条件概率分布。那么我们应该如何具体理解信息增益呢？我们首先计算事件A的信息熵，然后考虑事件B，计算在有事件B的情况下的条件熵，两者之差，就是信息熵的改变，改变越大，说明事件B对于事件A的相关性越大。由于涉及到信息论，所以信息量改变这一点，就变的略显抽象。</p>

<p>仿照上文中对卡方校验的介绍。利用信息增益解决问题的关键，在于找到这个条件概率分布，以计算条件熵。同样举文本分类中特征提取这个例子。事件A是指文档属于类ci，事件B是词w是否出现在文档中。根据P(Ci|w)和P(Ci|^w)这两个条件概率分布，可以计算条件熵H(C|T)。</p>

<p>不过仔细读过信息增益的概念，发现我们上面介绍的和KL散度差别挺大的，目前我也不能理解这两种概念是否本质上相同，但是在分类和挖掘领域，大家一般是像我说的那样去理解信息增益的。</p>

<p><a href="http://zh.wikipedia.org/wiki/%E4%BA%92%E4%BF%A1%E6%81%AF">互信息</a></p>

<p>在互信息定义的多条公式中，我一般使用第三个，就是X，Y熵之和减去联合熵。根据联合熵，就可以发现，决定互信息的，也是一个联合概率分布。</p>

<p>比较形象的理解互信息的公式方法是，认为事件X和事件Y相关，可以让总信息熵减少多少，差别越大，X和Y越相关。计算互信息的关键，是计算联合熵，因此在用互信息解决问题的适合，找到这个联合概率分布，是核心。不要试图去寻找其他的概率分布，实践表明，互信息的联合概率分布与卡方检验的非常相似，都是x是否怎样，y是否怎样。同样以文本分类为例，X为文档是否属于类c，Y事件为文档是否包含w，然后利用这个概率分布去计算联合熵。这么一看，和卡方检验几乎完全相同！只是利用了不同的理论基础。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[核函数理解]]></title>
    <link href="http://xutianming.github.io/blog/2013/07/20/understanding-kernel-function/"/>
    <updated>2013-07-20T11:13:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/07/20/understanding-kernel-function</id>
    <content type="html"><![CDATA[<p>SVM相关的理论知识看过几遍，但是感觉一直都不是理解的很透彻，特别是核函数的概念。今天又查阅了大家写的学习笔记等资料，结合自己的实践经验，感觉对核函数的理解更进一步了，所以记录在这里。</p>

<p>一句话说明核函数的意义和作用：为使非线性可分问题转化为线性可分，将特征向量映射到高维，映射函数所满足的性质。</p>

<p>具体说来，svm可以直接解决线性可分的分类问题，对于近似线性可分的问题，通过增加松弛变量，也可以实现分类。那么对于非线性可分的问题呢，比较直观的想法是映射，也就是通过把特征映射到高维，使非线性可分问题转化为线性可分的问题。但是就一般的实际问题来讲，寻找这个映射很困难，而且很难有可推广的方法。于是提出了kernel trick。可以绕过寻找这个映射函数。</p>

<p>在理论公式的推导中，可以算出参数w的表示方法：</p>

<p><img src="http://i.imgur.com/KqPVcLl.png?1" alt="Imgur" /></p>

<p>于是分类超平面的函数也可以写成：</p>

<p><img src="http://i.imgur.com/UVGLjMS.png?3" alt="Imgur" /></p>

<p>这个公式中只有&lt;x(i),x>的内积形式，也就是说，我们映射之后，需要计算&lt;Fi(xi),Fi(x)>的内积。而我们定义的核函数，就是这一内积的形式。</p>

<p><img src="http://i.imgur.com/lLicBOp.png?1" alt="Imgur" /></p>

<p>如此一来，我们就可以用核函数直接替换内积，相当于，我们把原始的特征向量，映射到了高维，并且避免了寻找映射函数。而libsvm中推荐使用的RBF核函数，就是一个有着良好特性的核函数。可以将样本映射到一个更高维的空间，可以处理非线性的分类问题。而其关键的参数c和g分别为惩罚因子和核函数的参数。（RBF 核K(x, y) = exp(－γ || x －y ||的平方),γ > 0，g为gamma，就是r）。我们在使用libsvm训练之前，应该首先寻找最优的参数c和γ。</p>
]]></content>
  </entry>
  
</feed>
