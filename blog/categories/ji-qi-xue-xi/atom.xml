<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 机器学习 | Xutianming Blog]]></title>
  <link href="http://xutianming.github.io/blog/categories/ji-qi-xue-xi/atom.xml" rel="self"/>
  <link href="http://xutianming.github.io/"/>
  <updated>2014-08-02T17:05:35+08:00</updated>
  <id>http://xutianming.github.io/</id>
  <author>
    <name><![CDATA[Xutianming]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[分布式机器学习算法框架的设计与实现]]></title>
    <link href="http://xutianming.github.io/blog/2014/03/02/hadoop-owlqn/"/>
    <updated>2014-03-02T14:52:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2014/03/02/hadoop-owlqn</id>
    <content type="html"><![CDATA[<p>最近一直在研究逻辑斯蒂回归算法。参考的是MSRA的Galen Andrew以及Jianfeng Gao的OWLQN算法。这一算法，是基于L-BFGS算法提出的。目标函数是带L1规范化的L2逻辑斯蒂回归损失函数，也就是说，即有L1也有L2，也可以只取其一。算法中，为了解决带L1规范化的目标函数在0点不可导的困难，定义了伪梯度。其他的步骤，与经典的L-BFGS算法大体一致。</p>

<p>这里给出几个参考链接，对于阅读论文有很大的帮助。</p>

<p><a href="http://en.wikipedia.org/wiki/Limited-memory_BFGS">L-BFGS</a></p>

<p><a href="http://en.wikipedia.org/wiki/Backtracking_line_search">Backtracking Line Search</a></p>

<p><a href="http://en.wikipedia.org/wiki/Wolfe_conditions">Wolfe Conditions</a></p>

<p><a href="http://en.wikipedia.org/wiki/Directional_derivative">Directional Derivative</a></p>

<p><a href="http://pan.baidu.com/disk/home">zjuOptimization课件</a></p>

<p>结合论文作者给出的单机LR的C++代码，可以比较快速的了解算法。但是我个人感觉原本的设计并不是特别好，C++的实现也比较一般，类的设计略显牵强。当然这是我从一个学习者的角度给出的评价，我代码读起来还很费劲的。</p>

<p>有了上述的知识以及对单机代码的掌握之后，我决定实现分布式的版本。基于Hadoop的MapReduce。首先分析，当数据量增大，所有的训练数据不足以存放在单机内存中时，算法的主要瓶颈在哪里。通过阅读代码可知，算法最耗时地方在于计算当前w下，所有训练样本的Loss的和，并更新梯度，也就是源码中的Eval函数。很自然的想法是，把数据分块，在每个Mapper上计算本样本的Loss和本样本对梯度更新的影响，在Reducer中对这些计算结果求和。这样，在LineSearch中，要进行多次的Eval，也就是每次迭代可能都要进行多次MR任务。这样的设计思路非常简单，但是存在很多限制，我会在博客的最后一一说明，这些改进将是下一步的工作方向。</p>

<p>下面主要说说我的设计。我说到单机C++版本的OWLQN设计的并不太好，我在我的设计中做了改进，也用了很多Java面向对象的特性。</p>

<ol>
<li>DifferentiableFunction接口。目标函数均为可导函数，所以当要扩展其他的算法时，只要实现此接口，本接口定义的功能有：函数求值、求某一点的梯度、求某一点在某方向的方向导数。</li>
<li>OptimizationSolution接口。定义最优化解法，本项目中为OWLQN，提供求解最优化的Minimize算法。当要扩展其他的解法时，比如梯度下降法，只要实现本接口就行。</li>
<li>IterationState抽象类。用于记录迭代中的信息，抽象类定义了一些基本的操作。</li>
<li>TerminationCriterion接口。迭代收敛的条件，用本条件和Tolerant做比较，小于Tol即为收敛。</li>
</ol>


<p>项目不大，但是Java面向对象的特性几乎都用到了。封装、继承、多态，面向接口和抽象类编程，即可在运行时根据传入参数的具体类型，调用对应的方法，非常灵活。</p>

<p>关于接口和抽象类，有很多相似点也有很多不同。它们都可以用来设计编程接口，让项目的扩展按照设计者的思路来，从语法层面限制扩展的方式。不同在于，接口更加灵活，因为Java类可以实现多个接口，但是只能继承一个类。类通过继承，有更强的限制性，并且使用继承的话，可以提供一些基本操作的实现，比如IterationState中的IterIncrement，增加迭代计数。</p>

<p>关于设计的改进，也就是我的设计与原项目不同在哪里呢？除了面向对象特性的应用外，主要的区别在于IterationState，原先的项目，把所有的计算，放在这个类里，并设置OWLQN类为FriendClass，非常的不合理，破坏了封装，逻辑上的划分也非常混乱。于是我在我的实现中，重新进行的划分和封装。</p>

<p>总的来说，新的设计，思路更加清晰并且扩展性也非常好。下一步，我计划将SVM也放入这个框架。</p>

<p>下一步的工作，主要是从本项目现有的缺陷出发。</p>

<p>本项目是基于Hadoop MapReduce的，有个非常大的限制，就是结点的IO。比较理想的情况下，我在进行每次迭代计算的时候，分到每个结点的数据集是不变的，变化的是当前的点X，那么这时候如果每个结点可以把分来的数据载入内存，每次迭代只根据不同的X进行计算，就可以避免本节点的磁盘IO，进而提高程序性能。但是MR框架，是不能避免磁盘IO的，每次都要读一遍数据。业界也早已发觉MR在分布式机器学习上的这一缺陷，于是有的基于MPI自己实现分布式，近两年也出现了新兴的开源的分布式计算框架Spark。这就是业界的方向了，我也会尽量的去了解下这些新兴的技术。</p>

<p>另外，在这个项目中，我设计的分布式算法还是很初级的，我的测试数据的维度并不高，那么我的训练数据矩阵按行处理，在和其他同学交流的时候，他们有提出，每一个训练sample也可以被划分，有可能会利用复杂的矩阵运算，来使得高维数据可以利用MR分布式计算。当然这只是一个初步的想法，具体的方向，还有待于进一步的阅读研究和请教。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[模型学习小结]]></title>
    <link href="http://xutianming.github.io/blog/2014/03/01/models-in-perspective-of-math/"/>
    <updated>2014-03-01T22:16:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2014/03/01/models-in-perspective-of-math</id>
    <content type="html"><![CDATA[<p>近来我对机器学习的模型有了一些更接近本质的认识。也让机器学习的这些理论，在我的眼里，变得不那么神秘。</p>

<p>机器学习的理论，可以分为两个部分，一部分是建模，一部分是求解模型。所有的机器学习算法，逻辑斯蒂回归、支持向量机，本质上来讲，都是一套建模的方法和一套可行有效的解法。</p>

<p>建模，从数学的角度，是对实际问题的描述。比如逻辑斯蒂回归利用逻辑斯蒂分布的特殊性质建模，也就是odds ratio为线性函数。而odds ratio是本sample为正例和负例的概率的比值的log。这样odds ratio的正负，可以代表本sample为正例的概率大还是为负例的概率大。而odds ratio的正负，就是线性函数的正负。因此有了逻辑斯蒂回归。建模的任务是描述问题，并得出目标函数，也即是Loss Function。Loss Function经常利用极大似然估计（Max likelyhood）和最小平方误差（Least squares）写出。支持向量机也是如此，支持向量机的建模过程中，认为sample到分类超平面的距离，衡量分类的置信程度。那么定义符合要求的分类超平面为，数据集中所有的点，到本分类超平面的距离，都大于等于R。然后极大化R，得到目标函数。</p>

<p>得到目标函数之后，就要求目标函数的极值。数学里的最优化问题，有非常多的方法。常见的有梯度下降法、牛顿法、拟牛顿法。理论上来讲，求解和建模是独立和分开的，也就是说，上面建模得到的目标函数，可以用任意一种最优化解法来解。当然这只是最理想的情况。实践中，每一种模型（目标函数），几乎都对应了一种实际可行的，可计算的解法。</p>

<p>从这种比较高层次的角度，再去读那些机器学习的书籍，会更加从容，应该可以把握的更好。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[行业关键词分类整理2]]></title>
    <link href="http://xutianming.github.io/blog/2013/11/14/text-classification-step-by-step2/"/>
    <updated>2013-11-14T12:45:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/11/14/text-classification-step-by-step2</id>
    <content type="html"><![CDATA[<p>在上一篇博客里，详细讲述了项目的具体过程。项目结束之后，从昨天开始，我就开始着手去查看一些资料，整理思路，看看项目的过程中，哪些是有理论依据、切实可行；哪些只是基于个人经验的推论，可能存在很多的纰漏。从而能找到一些提高的可能性，同时也梳理一下整体的思路，在以后遇到问题的时候，可以有的放矢。知道哪些地方可以不再进行尝试了，哪些地方需要着重考虑。</p>

<p>下面的思路整理，是参考了《信息检索导论》书中，对于文本分类的介绍，并且结合了这次项目的经历。书是2010年出版，偏向综述性质，也兼顾了一定的细节。毕竟只参考了一本书，不敢说尽善尽美，以后如果有读到更好的、更全面的，会再来补充的。</p>

<p>首先是文档的表示形式，有两种，一种是向量空间模型（VSM），目前我所使用的，全部都是利用向量空间模型来做的，即用特征向量来表示文档。第二种是，潜语义空间，是自然语言处理领域的，接下来，我会研究和尝试下这个方面，目前了解很少，就不多讲了。</p>

<p>下面的解决方案，我都是在VSM的基础上理解的，也许也能适用于潜语义空间。</p>

<p>总体来说，一个高准确率的分类系统，是由一个自动分类器和一系列人工撰写的布尔规则来组成的。前者由数据通过机器学习得来，后者由领域专家人工撰写。这一点，在我们的项目过程中，我大部分时间都是追求分类器的高准确率，基本上没有花很多时间去做规则。</p>

<p>在面对一个文本分类项目的时候，首先要关注的就是训练数据：</p>

<ol>
<li>假如没有标记好的训练数据，而有领域专家的话，采用人工编写规则的方法。这种方法虽然要花很多人力，但是往往分类的效果并不差，往往有高的准确率和召回率。</li>
<li>如果拥有的训练数据非常少。应该采用高偏差低方差的分类器，比如NB。而像KNN这种对训练数据拟合比较好的，低偏差高方差的分类器，就效果不太好。对应的如果我们使用SVM，参数C倾向于取的小一点。另外除了利用现有的训练数据外，通过半监督学习的方式，来增加训练数据，也是一个思路。主动学习系统，就是建立一个系统来确定需要标注那些文档。</li>
<li>如果拥有比较多的已经标注的数据（我们的这个项目就是这种情况）。那么分类器的选择，对于最终的效果就没有太大的影响。这种情况下，一般使用SVM，因为很多实验表明了，SVM是同等条件下来说，效果比较好的分类器。在我们的项目中，就有这种情况，我们尝试了NB和线性核的Liblinear，都没有本质的提升，准确率都在80%~90%之间。</li>
</ol>


<p>在上面的第三种情况下，分类器的选择并不重要，那么如果提高分类的效果呢？</p>

<ol>
<li>撰写人工规则。这对于修正某一特定类的误伤，比较立竿见影。指望这个，大幅度提高准确率，感觉还是要有很多工作要做的。</li>
<li>使用领域特征。一个普遍的事实，采用领域相关的文本特征，在性能上会比采用新的机器学习方法获得更大的提升。</li>
<li>采用复杂的分类体系。比如层次系统，比如使用弱分类器的组合，Voting、Bagging、Boosting等方法。再辅助以人工审核，将低分类置信度的数据，纳入审核系统。</li>
</ol>


<p>下面详细说一下，其中的第二条，也就是领域特征的选取。</p>

<p>对文本分类问题，如果对待特定的问题加入合适的额外特征，分类的效果还能显著提升。这一过程往往称为特征工程，目前来说，特征工程还是主要依赖人的技巧而不是机器学习的结果，好的特征工程往往可以大幅度提高分类器的效果。（写到这里我想，那些在项目中能有95%以上准确率的，应该都是花时间做了特征工程的。到时候公布冠军的代码的时候，可以验证一下。）</p>

<p>除此之外，还有一些技巧。</p>

<ol>
<li>将特殊字符串替换成更知名的词条。举例来说，在化工类中，可能出现氯化钠、氧化钙等名词，假如没有类似的类的话。这些应是非常有区分度的特征，但是在分类过程中，首先分词器可能无法识别这种词，其次用chi-square也许能提取出这种特征，但是在tf-idf赋权的时候，他们的权重也不可能高（因为不会出现太多次）。假如把这些词统一替换成，“化学物质”类似的更知名的词条，那么化学物质就能是个很好的特征了。</li>
<li>有时候，词的部分和词的多词模式，也许是很好的特征。这很类似于我们在项目采用的2-gram、3-gram组合词。总的思路是先通过好的方法，去发现一些好的多词模式的候选，加入特征集之中，再利用常规方法去选择特征。具体用什么方法去挖掘好的多词模式以及如何使用。就目前来说，对我都是一个疑问。接下来，我也会有针对的性的读一些这方面的资料。</li>
<li>基于文章结构。比如特征出现在标题中，特征的权重翻倍。新闻类文章，特征出现在第一段或者每段的第一句话中，增加权重之类的。这就是基于位置的特征选择方法。在之前的Wikipedia的项目中，有一些类似的情况。</li>
</ol>


<p>综上可知，大致可做的工作就这么些。基本上对于每个文本分类项目，都有自己的特性，也许整体思路都是一样的。但是各个环节也都影响着最终的结果。结合我们的这个行业关键词分类的项目来看，我们可做的工作有：</p>

<ol>
<li>撰写人工规则。这个虽然可以提升效果，获得好成绩，但是目前阶段来讲，我更倾向于直接从别人那里获得做这个工作的最佳实践而不是自己去花大量的时间去实践。所以这个方向暂时不考虑，可以读一些方法性的文章。</li>
<li>领域特征选择。如何去做特征工程？如何可以系统的去选择领域特征。这是个需要研究的方法性问题。</li>
<li>如何选择和使用多词模式。凭借目前对这份数据的浅薄的理解，我们提出了一些方法，但是实践证明效果不好。我们首先，组合出所有可能的2-gram，然后利用2-gram出现的频率，以及2-gram的组成单元出现次数的比例，来筛选2-gram。选出来之后，我们就去根据这些2-gram去合并了原始的中文分词结果。这样相当于我们可能会淹没掉2-gram的每个组成单元的，可能的高区分度。这种方式显然不好，所以需要通过更多的阅读，来获得这方面的确定性的知识。而不是自己胡乱尝试。</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[行业关键词分类项目总结1]]></title>
    <link href="http://xutianming.github.io/blog/2013/11/14/text-classification-step-by-step/"/>
    <updated>2013-11-14T10:24:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/11/14/text-classification-step-by-step</id>
    <content type="html"><![CDATA[<p>在过去的一个月里，完成了一个行业关键词分类的项目，100w的训练数据，1000w的测试数据，33个分类。项目期限已经到了，但是我们的准确率目前只有88%，尝试了很多方法，遇到很多困难，也走过弯路。</p>

<p>项目结束并且效果不好，自然要寻找原因。一方面是等待比赛完全结束，公布冠军的解决方法，来开阔下眼界；另一方面就是更广泛的读一些相关的资料，做一些在项目过程中没有精力去做的阅读和学习工作。</p>

<p>首先简单描述下项目的过程。这次项目提供了33个分类，和一些要分类的关键词。每个关键词，提供了10个搜索引擎的自然排序的结果，以及购买这些关键词的用户。一部分已经标注的训练数据，和大量要分类的测试数据，分类效果的评估用的是宏平均(Macro-average)准确率。接下来，我们做了如下尝试：</p>

<ol>
<li>利用提供的文本数据进行文本分类。其中包括了利用lucene的IKAnalyzer进行分词、利用Chi-square进行特征选择、利用tf-idf作为权重生成文本的向量表示形式、利用Liblinear进行线性核的分类。第一次准确率87%。由于以上工作都是在Hadoop上完成的，加上学习和适应Hadoop平台的时间，其实只要一天的工作量的事情，做了2周（比赛承办方提供的Hadoop集群的限制实在是太多了）。</li>
<li>利用一些人工制定的规则，挖掘一些有着高区分度的词，利用这些词去修正上述文本分类的结果。这一步，把准确率提高到了88.5%。这也是我们最好的成绩了，本来可以利用这种方法获得更多的准确率的提升的，但是感觉可学习的东西很多，不想把更多的时间放在对数据的理解上。</li>
<li>利用关键词的用户购买信息去做关键词分类。这样做是建立在“购买关键词的用户相似，那么关键词倾向于属于同一类”这一假设的基础上的。所以想对&lt;关键词-用户>向量，应用分类算法。这一矩阵存在着维度过高（600w维）和过于稀疏的问题（很少有两个用户买同一个关键字）。所以推断直接分类既不可行，效果也不好。</li>
<li>这一条详细讲一下用户矩阵的使用。首先想到的是对用户聚类，以降低&lt;关键词-用户>向量的维度和稀疏性，使用k-means。但是由于上面所说的稀疏性问题，所以聚类效果也不好。于是想到利用第一次文本分类的结果，补充训练数据，另外首先对关键词聚类（已经聚好了，33类），这样&lt;用户-关键词>向量维度33维，并且不稀疏。可以使用k-means对用户进行聚类了，分别制定k为100和1000，进行聚类。在使用的过程中，发现了一个问题，k-means一般是基于距离的，而在这个项目的用户中，和质心距离相等的两个点，并不一定相同，距离计算过程中把所有的维度（关键词类别）等同看待，而现实中，我们购买了不同维度（类别）的关键词的用户，虽然距离相等，但是是属于不同类的。基于上述思考，我放弃了使用k-means，按照自己的想法，对用户进行了聚类。得到了2w类左右的用户。用户聚类之后，就达到了对&lt;关键词-用户>矩阵的降维和去稀疏的目的。可以应用分类算法了。</li>
<li>利用上述的方法，利用用户对关键词分类。但是结果非常不理想。原因在于，我们在第3步里的假设是不对的。也就是说，假如几个关键词，被几个相同或者相似的用户购买，并不代表关键词属于同一类。分析bad case可以看出，比如公司转让，服装公司转让被分入了服装类，房地产公司转让被分入了房地产类。但是这些关键词应该是被一个公司转让中介性质的用户购买了。所以从用户角度去分类，是不可行的。</li>
<li>经过上面的尝试，时间已经过去了3周。我们放弃了使用用户购买信息，试图提高文本分类的效果。首先我们发现，需要利用多词模式去提高特征质量，比如，搬家是个特征词，公司不是，但是搬家公司，是个非常有区分度的特征。由于没有事先的积累，也找不到现成的工具，我们自己根据经验和对数据的理解，写了bi-gram和tri-gram的map-reduce组合算法。使用频率对多词模式进行筛选，然后加入特征集训练。这一步，问题很多，因为大部分是拍脑袋定的，感觉很不科学，接下来，我可能会对这方面做一些针对性的阅读。</li>
<li>重复了文本分类的步骤，做了第二版的分类器，但是由于多词模式的筛选和使用都有些问题，所以这一版分类器没有取得预期的进步。</li>
<li>时间到了最后一周，感觉也没有什么特别好的点子了。偶然之中发现了Multi-class 分类相关的知识，比如one-vs-rest，one-vs-one等等。以为发现了新的方法，于是我们用Hadoop又做了一个one-vs-rest的多类分布式分类器，主要的思路是参考的别人的，花了很多时间去读别人的代码以及了解liblinear的各项接口和参数。这段时间算是对SVM和Hadoop编程有了更进一步的知识。无奈，最终发现，其实本来Liblinear做多类分类，就是用的one-vs-rest。而且我们的分类器，最终效果也不好。</li>
</ol>


<p>这就是过去一个月中，我们进行的所有工作。学到了很多知识，对SVM，NB分类器，对Hadoop的java接口和Streaming以及map-reduce的思想都有了进一步的认识，并且可以顺利使用了。虽然最终结果并不好，但是对于缺乏经验的我们来说，算是一个很好的经历。下篇文章里，我再结合一些书本上的知识，讲一些关于这个项目的思考，和可能的改进方向。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MapReduce初试]]></title>
    <link href="http://xutianming.github.io/blog/2013/10/29/mapreduce-program-trial/"/>
    <updated>2013-10-29T20:46:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/10/29/mapreduce-program-trial</id>
    <content type="html"><![CDATA[<p>最近在尝试做一个行业关键词分类的项目，训练数据百万级，测试数据千万。数据量不大，但是我的机器跑来已经相当吃力了，于是想用一些简单的Hadoop知识，体验一下这个目前业内最流行的分布式计算框架。</p>

<p>以前接触过很多分布式计算的知识，也学习过MapReduce。甚至实习期间，还多少使用到了hadoop的hive。但是一直都流于表面和概念，没有机会实践，甚至都不知道一个MapReduce程序如何来写，这次终于有机会尝试。</p>

<p>体验的过程并不顺利，为了简单起见，我使用Python和Streaming。参考网上的博客，很快就写出了MapReduce程序。这里的心得是，编写MapReduce程序并不比普通的编程难，关键是把算法分布式化，思考清楚，在Map和Reduce的过程中的数据流。这里分别附上我写的简单的Map，Reduce程序，作为以后工作的模板。</p>

<p>Mapper.py</p>

<pre><code>#!/usr/bin/env python
import math
import sys

features = []

fin = open("feature.txt","r")
for line in fin:
    winfo = line.split("\t")
    word = winfo[0].strip()
    features.append(word)
fin.close()


for line in sys.stdin :
    document = []
    info = line.strip().split("\t")
    for i in range(2,len(info)):
        document.append(info[i].strip())
    for feature in features:
        if feature in document :
            print '%s\t%s' % (feature, 1)
</code></pre>

<p>Reducer.py</p>

<pre><code>#!/usr/bin/env python
import math
import sys

current_word = None
current_count = 0
word = None

for line in sys.stdin:
    line = line.strip()
    word,count = line.split('\t')

try:
    count = int(count)
except ValueError:
    continue

if current_word == word:
    current_count += count
else :
    if current_word :
        print '%s\t%s' % (current_word,current_count)
    current_count = count
    current_word = word

if current_word == word:
    print '%s\t%s' % (current_word, current_count)
</code></pre>

<p>这一个统计DocumentFrequence的MapReduce程序。下面就一一描述下在编写这个程序中遇到的问题和解决方案。</p>

<ol>
<li>首先注意到Mapper中需要读入一个文件。一开始不懂文件应该怎么才可以被程序读到。尝试过放在HDFS和改变各种文件路径。其实只要在执行hadoop任务的时候，用<code>-file</code>参数指定就好了。</li>
<li>关于失败提示。<code># of failed Map Tasks exceeded allowed limit</code>,一开始的时候总是遇到。出现这个job失败提示的原因在于mapper程序有问题。执行失败，超过限度，job就会被杀死。所以mapper要做好本地测试。使用<code>cat input|python mapper.py|sort|reducer.py</code>可以对程序进行基本的测试。再就是在本地的hadoop平台上先跑一下，假如说没问题了，可以跑个5%，一般就可以提交到线上的集群了。</li>
<li>关于Streaming命令的各个参数。除了使用<code>-mapper</code>，<code>-reducer</code>之外。还可以利用<code>-D</code>选项，指定一些hadoop的运行参数，比如mapper和reducer的数量。这里有个比较重要的技巧。就是假如我们的执行逻辑中，只需要mapper不需要reducer。有几种办法，一个是不指定reducer，默认使用<code>IdentityReducer</code>，mapper的输入直接转化为reducer的输出；二是设置reducer数量为0，<code>-setReduceTasks=0</code>，有几个mapper即有几个输出文件。</li>
<li>最重要的一点，程序效率。上面给出的两个模板很高效。9GB的输入文件，十分钟即可在线上集群中完成。但是一开始并不是这样的。这个问题困扰了我整个周末。一开始，我想当然以为既然hadoop了，程序效率就不是短板了，我效率低，mapper分块小一点也很快完成，于是编程序的时候就很不在意效率。用list查找什么的，各种遍历循环，怎么好写怎么来。于是一开始我的程序测试好提交之后，job执行很慢，可以理解。令人崩溃的是，总是莫名其妙失败，我一度认为是线上集群不稳定弱爆了。后来在乐乐的提示下改进了程序的效率，list改用了hash，提高查找效率，去掉了几个循环。于是job很顺利也很迅速执行完了。事后分析，我觉得线上的集群可能做了很多的限制，我之前的写法，由于消耗太多的资源，由于本身优先级不高，就总是被delay，就会有莫名其妙的失败。所以mapreduce程序中，效率也是非常重要的，每一个task都高效执行，那么整个job都能很快完成，大大提高效率，节省时间啊！终于深刻认识到程序效率的重要性了。</li>
<li>最后一点，我还没有很好的解决。就是mapreduce程序的调试。hadoop实战这本书上有介绍debug的方法，我没怎么看懂，另外简单的程序，使用print大法打印一些调试信息和程序运行信息，很有利于快速定位bug。目前还不知道如何打印。如果在本地的hadoop的话，可以输出到<code>stderr</code>上，会最终输出到logs目录下的日志里。但是线上集群的话，就没有结点的访问权限，自然没法查看日志。所以我就想能否通过<code>Reporter</code>来打印一些消息。还没有尝试过，需要实践来验证。</li>
</ol>

]]></content>
  </entry>
  
</feed>
