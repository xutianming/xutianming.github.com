<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 机器学习 | Xutianming Blog]]></title>
  <link href="http://xutianming.github.io/blog/categories/ji-qi-xue-xi/atom.xml" rel="self"/>
  <link href="http://xutianming.github.io/"/>
  <updated>2013-09-28T16:17:21+08:00</updated>
  <id>http://xutianming.github.io/</id>
  <author>
    <name><![CDATA[Xutianming]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[知识可视化项目第一阶段总结2]]></title>
    <link href="http://xutianming.github.io/blog/2013/09/24/summary-of-wikivis-project-2/"/>
    <updated>2013-09-24T21:15:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/09/24/summary-of-wikivis-project-2</id>
    <content type="html"><![CDATA[<p>今天和乐乐讨论了发表了Wikipedia Miner思路的论文，感觉颇深，故在此记录一下。</p>

<p>今天其实并没有讨论太多论文的内容，主要是基于项目第一阶段的反思，所以感觉还是很有意义的。这样的话，可以一步一步的提高我们解决问题的能力。关于面对一个要解决的课题的时候，我们从什么角度去思考它的解决方案，以及在目前的解决方案遇到了问题，走入了死胡同，如何调整路线，以最终找到那条正确的道路。</p>

<p>第一个问题是关于思考问题的角度。很明显，在第一阶段的项目进展中，我们并不成功，没有找到一种泛化能力非常好的算法，来对维基百科的数据进行有效的挖掘。在今天读了论文之后，发现论文作者和我们对于问题的思考思路是完全不同的。具体来讲，论文的课题是要解决“给哪些词加链接，有助于读者理解当前的文章”，而我们的目的是“有哪些关键字，可以很好的代表这篇文章，我们要给出这些关键字对应的维基百科链接”（这里仅指我们项目中的关键词打分步骤）。从这个角度看，我们要解决的问题其实是一致的，但是我们分别采用了什么样的思路来思考问题的呢？首先讲论文中的思路，维基百科中现有的有链接的文章是标注好的语料，那么满足什么条件的词是重要的关键字，需要加链接呢？我可以利用机器学习的方法来训练一个模型。首先根据经验选择一些特征，再利用有标注的训练数据，选择合适的算法，这样就可以学习模型了。这个模型告诉我们，在一篇文章里，那些词是关键字。那么，我之前使用的思路是什么呢？在语料充足的情况下，我们有很多统计量，比如卡方、平均值、标准差、余弦距离等等。那么用哪个统计量去描述关键词的特征最好呢？于是自然而然的想到了关键词提取，根据前人论文中提出的方法实现了。</p>

<p>这是两个解决问题的思路。不能说两个思路哪个好哪个坏，因为没什么可比性。这两种方法，前者立足于学习筛选关键字的标准，而后者立足于，我用什么样的标准，可以筛选出关键字。但是实践证明，解决我这个项目的问题，前者更好（Wikipedia Miner关键词挖掘效果很好）。因为在我的方法中，有个致命的弱点，就是语料。我是从一篇文章中根据统计规律去提取关键字，每个主题，使用什么词、什么词关键都满足一定的统计规律。那么语料不足怎么办？争取在不改变统计规律的前提下扩充语料，这一点太难做到了。而论文中的方法呢，学习的是标注关键字的方法，假定不论什么主题，哪些关键字重要，都是满足一定的准则的。显然后者更加符合维基百科的特点，详见<a href="http://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking">维基百科规则</a></p>

<p>下面讨论第二个问题，其实是个知识面的问题。因为最终乐乐发现，在我们读的这篇论文描述前人工作的时候提到了关键字提取相关的技术，在一篇综述性的文献中。而论文的作者，正是在前人工作的基础上，提出了这种方法，在2008年。按理说过去这么久了，为什么我们在最初确定技术方案的时候，没有找到现在这个更好的解决方案呢？因为我们使用的谷歌学术中没有收录那篇文章，并且在最初我们对于这个领域比较陌生的时候，很难使用准确的关键字去在搜索引擎上搜索，所以也就比较难一下子就找到最好的解决方案。也是经过了一段时间的研究和摸索，机缘巧合下，我的导师给我推荐了Wikipedia Miner。我才发现我们的问题已经有了这么优秀的解决方案了。</p>

<p>仔细想想呢，这其实涉及到了领域知识的问题。用什么技术，解决什么问题，是要经过长期实践积累经验的。随着阅历的提升，知道的办法越多，就越有解决问题的能力。在这一点上，我的导师自然大大的超越了我们。不过如何更有效的利用各种学术资源，会议门户啊、学术搜索啊、学术数据库等等，回头我还是要好好向我的导师请教一下。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[卡方校验、信息增益和互信息]]></title>
    <link href="http://xutianming.github.io/blog/2013/09/14/chi-square-information-gain-and-mutual-information/"/>
    <updated>2013-09-14T20:51:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/09/14/chi-square-information-gain-and-mutual-information</id>
    <content type="html"><![CDATA[<p>从一开始做文本分类到现在做的项目，一直在不断的学习新的知识，练习已经掌握的工具，也越来越深刻的认识到，“你所掌握的机器学习方法，并不是死板的一成不变的算法，而是一个一个的小工具，是解决问题的思路”。</p>

<p>这篇博客将对比三个在我进行文本挖掘相关工作的时候经常用到的小工具，以及我对他们的认识，仅限个人的粗浅观点，也是我当前的认识，不见得正确。</p>

<p><a href="http://zh.wikipedia.org/wiki/%E7%9A%AE%E7%88%BE%E6%A3%AE%E5%8D%A1%E6%96%B9%E6%AA%A2%E5%AE%9A">卡方校验</a></p>

<p>首先理解卡方校验的概念，除了维基百科，我总会看<a href="http://www.blogjava.net/zhenandaci/archive/2008/08/31/225966.html">这篇博客</a>。简单来讲，卡方校验是用来判别两个事件是否相关的方法，因此在特征提取、关键词提取中都有很多的应用。基本思路是，首先假设A、B不相关，然后根据A和B的联合概率分布，计算卡方值，公式在定义中都有给出，卡方值越大，越倾向于推翻原假设，也就是A、B越相关。在计算卡方值的过程中，最关键的问题是找到联合概率分布。那么我们应该如何理解卡方值呢？卡方值的含义其实是这样的，我们首先假设A、B不相关，也就是AB相互独立，那么我们可以为联合概率分布中的AB同时成立的事件计算期望值，根据定义公式显示，卡方值是这个期望值和实际值之间的差的平方除以期望值的和（定义和方差很像！），衡量了这两个量拟合的程度。那么，卡方值越大，代表期望值和实际值之间的差越大，也就是我们计算期望值所依赖的假设，是不成立的！</p>

<p>当我们想用卡方检验的方法，去判断两个事件是否相关的时候，理解问题的过程，就是建立事件A、B联合概率分布的过程，找到这个概率分布，就可以计算卡方值。举例说明的话，在文本分类的特征提取这一步，事件A是特征词w是否存在在本篇文档中，事件B是本篇文档是否属于类c。首先假设“w不是类c的特征词”，那么词w在属于类c的所有文档中出现的概率，应当与w在全部文档中出现的概率相同，我们依据这一假设，可以算出w出现在c类的文档数的期望，用期望值和实际值的差的平方除以作为卡方值。</p>

<p><a href="http://zh.wikipedia.org/wiki/%E7%9B%B8%E5%AF%B9%E7%86%B5">信息增益</a></p>

<p>也经常作为特征提取的方法，也用来衡量两个事件的相关性，但是是从信息论的角度来的。信息论用熵来衡量信息量。信息增益，简单理解就是在已知条件B的情况下，事件A的信息量的改变程度，就是熵的改变程度。同样有<a href="http://www.blogjava.net/zhenandaci/archive/2009/03/24/261701.html">一篇博客</a>讲得很透彻。从上面的描述可以看出，计算信息增益的关键是计算熵和条件熵。所以说，信息增益对应的概率分布，是普通的概率分布和一个条件概率分布。那么我们应该如何具体理解信息增益呢？我们首先计算事件A的信息熵，然后考虑事件B，计算在有事件B的情况下的条件熵，两者之差，就是信息熵的改变，改变越大，说明事件B对于事件A的相关性越大。由于涉及到信息论，所以信息量改变这一点，就变的略显抽象。</p>

<p>仿照上文中对卡方校验的介绍。利用信息增益解决问题的关键，在于找到这个条件概率分布，以计算条件熵。同样举文本分类中特征提取这个例子。事件A是指文档属于类ci，事件B是词w是否出现在文档中。根据P(Ci|w)和P(Ci|^w)这两个条件概率分布，可以计算条件熵H(C|T)。</p>

<p>不过仔细读过信息增益的概念，发现我们上面介绍的和KL散度差别挺大的，目前我也不能理解这两种概念是否本质上相同，但是在分类和挖掘领域，大家一般是像我说的那样去理解信息增益的。</p>

<p><a href="http://zh.wikipedia.org/wiki/%E4%BA%92%E4%BF%A1%E6%81%AF">互信息</a></p>

<p>在互信息定义的多条公式中，我一般使用第三个，就是X，Y熵之和减去联合熵。根据联合熵，就可以发现，决定互信息的，也是一个联合概率分布。</p>

<p>比较形象的理解互信息的公式方法是，认为事件X和事件Y相关，可以让总信息熵减少多少，差别越大，X和Y越相关。计算互信息的关键，是计算联合熵，因此在用互信息解决问题的适合，找到这个联合概率分布，是核心。不要试图去寻找其他的概率分布，实践表明，互信息的联合概率分布与卡方检验的非常相似，都是x是否怎样，y是否怎样。同样以文本分类为例，X为文档是否属于类c，Y事件为文档是否包含w，然后利用这个概率分布去计算联合熵。这么一看，和卡方检验几乎完全相同！只是利用了不同的理论基础。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[核函数理解]]></title>
    <link href="http://xutianming.github.io/blog/2013/07/20/understanding-kernel-function/"/>
    <updated>2013-07-20T11:13:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/07/20/understanding-kernel-function</id>
    <content type="html"><![CDATA[<p>SVM相关的理论知识看过几遍，但是感觉一直都不是理解的很透彻，特别是核函数的概念。今天又查阅了大家写的学习笔记等资料，结合自己的实践经验，感觉对核函数的理解更进一步了，所以记录在这里。</p>

<p>一句话说明核函数的意义和作用：为使非线性可分问题转化为线性可分，将特征向量映射到高维，映射函数所满足的性质。</p>

<p>具体说来，svm可以直接解决线性可分的分类问题，对于近似线性可分的问题，通过增加松弛变量，也可以实现分类。那么对于非线性可分的问题呢，比较直观的想法是映射，也就是通过把特征映射到高维，使非线性可分问题转化为线性可分的问题。但是就一般的实际问题来讲，寻找这个映射很困难，而且很难有可推广的方法。于是提出了kernel trick。可以绕过寻找这个映射函数。</p>

<p>在理论公式的推导中，可以算出参数w的表示方法：</p>

<p><img src="http://i.imgur.com/KqPVcLl.png?1" alt="Imgur" /></p>

<p>于是分类超平面的函数也可以写成：</p>

<p><img src="http://i.imgur.com/UVGLjMS.png?3" alt="Imgur" /></p>

<p>这个公式中只有&lt;x(i),x>的内积形式，也就是说，我们映射之后，需要计算&lt;Fi(xi),Fi(x)>的内积。而我们定义的核函数，就是这一内积的形式。</p>

<p><img src="http://i.imgur.com/lLicBOp.png?1" alt="Imgur" /></p>

<p>如此一来，我们就可以用核函数直接替换内积，相当于，我们把原始的特征向量，映射到了高维，并且避免了寻找映射函数。而libsvm中推荐使用的RBF核函数，就是一个有着良好特性的核函数。可以将样本映射到一个更高维的空间，可以处理非线性的分类问题。而其关键的参数c和g分别为惩罚因子和核函数的参数。（RBF 核K(x, y) = exp(－γ || x －y ||的平方),γ > 0，g为gamma，就是r）。我们在使用libsvm训练之前，应该首先寻找最优的参数c和γ。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[文本分类实践]]></title>
    <link href="http://xutianming.github.io/blog/2013/07/19/text-classification-in-action/"/>
    <updated>2013-07-19T22:14:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/07/19/text-classification-in-action</id>
    <content type="html"><![CDATA[<p>学习机器学习有一阵子了，先是仔细阅读了《统计学习方法》这本书，对目前比较流行的统计学习方法和模型有了大致的认识，比如朴素贝叶斯、逻辑斯蒂回归、支持向量机、隐马尔科夫模型等等。但是仅仅有理论上的认识是不够的，关键还是指导实践解决问题。</p>

<p>先前在百度实习的时候，已经对机器学习有了初步的认识，并且当时也利用逻辑斯蒂回归训练了一个还算不错的模型，虽然最终由于时间原因，没有做成一个可以应用到线上的模块，但是对于我对机器学习的这第一次尝试，我还是很满意的，当时也决定在研二的这一年里，好好研究一下机器学习相关的东西，以后好找相关方向的工作。</p>

<p>最初的实践决定从文本分类开始，因为这个领域相对来说研究的比较成熟，并且在工业界仍然有很多很重要的应用。由于svm一直是个非常重要的机器学习模型，之前对原理有一定的了解，于是决定从使用svm开始。网上相关的教程很多，我也是慢慢参考着别人的方法一步一步尝试。这里主要记录下我实践的过程，以供日后参考。</p>

<p>文本分类大致分为以下几步：</p>

<ul>
<li>数据预处理（一般是分词，去停）</li>
<li>特征选择（比较好的方法有卡方校验和信息增益）</li>
<li>特征值确定（比较好的是tf-idf值）</li>
<li>特征向量计算</li>
<li>模型训练</li>
<li>模型测试</li>
</ul>


<p>我使用的是复旦大学自然语言理解实验室提供的预料，选取了computer、agriculture、sports和history，选择分类的时候特意选择了差别比较大的几类，这样可以在某种程度上提高分类效果吧。</p>

<p>首先是数据预处理，我使用了同事推荐了一个分词工具，分词的效果不是很好，算是个初级的分词工具，基于python的，使用起来非常简单。分词之后，把预料中的文章，处理成了一个词一行的文档，便于下一步的处理。</p>

<p>数据预处理之后，就是选择特征了，这里所谓的“特征”，就是上一步分出来的一个一个的词，每个词都可以说是一个特征，但是把每个特征都哪来使用，显然是不现实的，太高维的特征向量，首先训练起来比较费时，另外会使模型复杂度提升从而容易使svm训练过拟合。我训练的时候，本想使用1w维的特征，最终使用了7600左右的特征，具体的我后面会说。考虑到以上原因，所以要有个特征选择的过程，也就是选择那些可以很好的区分每一类的词，比如包含了“农业”一词的文章，属于农业类的可能性比较大，所以这样的词，就算是好特征。那么如何筛选出这些特征呢，要有个数学上的量，来衡量特征的好坏，可供选择的有互信息、信息增益、卡方校验等方法。其中互信息的效果比较差，我就没自己尝试，我先后使用了信息增益和卡方校验的方法。</p>

<p>卡方校验，学名是“皮尔森卡方检验”，是一种假设检验方法。首先，我们的假设是“词word和类class不相关”，然后针对包含word的文档和以及属于class的文档的联合分布，算卡方值，每类选择卡方值最大的2500个值，去作为特征值，因为卡方值越大，倾向于原假设为假，也就是说，卡方值越大，词word和类class越相关，这个词就是我们要找的好特征。为啥是2500个值，因为我初步决定选择1w个特征么，一共四类，每一类选择2500个。最终计算下来，因为分词效果的原因，很多单字的特征被筛选出来，但是单字的信息非常少，我觉得不足以作为一个好的特征，所以我对于筛选出来的1w个特征，去除了那些单字的词，就剩下了大约7600个词作为特征。</p>

<p>特征选择出来之后，我们要利用这些特征去表示要分类的文档，也就是所谓的“把文档映射到特征空间”，用向量来表示文档。这里会遇到一个问题，如何用特征表示文档，另外每个特征对于一个文档都是等同重要的嘛？这里就自然涉及到为特征赋权重。我是用的是tf-idf值。选择这个的原因是，卡方校验有个缺陷，就是“低频率缺陷”，那些在一篇文档里只出现过一次的词，也有可能会有很大的卡方值，但是这个词，对这篇文章可能没太大价值，那些只在某一类文章中大量出现，而在其他类中很少出现的词，才是决定类别的词。而我个人认为tf-idf值可以比较好的弥补这一缺陷。当然我只是定性分析，我不太懂数学上的证明，也没去查阅相关的资料。</p>

<p>如此一来，我们把文档用特征向量来表示了，接下来就是训练模型了。在上述过程的实现过程中，我没有遇到太大的问题，unix+python是很有利的文本处理工具，实现起来很快。我是用的svm工具是libsvm，使用也很简单，但是我确实在训练模型的时候遇到了一个问题。</p>

<p>一般来讲，libsvm的使用流程是这样的：</p>

<ul>
<li>svm-scale，对数据进行归一化处理，映射到[0,1]区间内</li>
<li>grid.py 寻找最优参数c和g</li>
<li>svm-train</li>
<li>svm-predict</li>
</ul>


<p>我第一次使用的时候，用信息增益选择特征，libsvm不太懂，也没去查太多的资料，略去了第二步，也就是我训练的时候没指定参数。这训练效果太差了，把所有的文档都分到了一类中。一开始我总怀疑特征提取的不好，于是换卡方校验，结果还是这样，之后开始怀疑我训练的方法不对，于是很快发现了问题。在指定c和g参数之后，训练结果非常好，对训练数据的准确率为100%，在测试集上的准确率为93.52%，比人家论文上的准确率还高，非常让人振奋。</p>

<p>至此，算是完满的完成了一次svm文本分类的实践，结果也符合之前给自己定的目标90%以上的准确率。接下来，我会根据这次实践的结果，总结分析，再回归到svm的理论上，比如c和g具体含义对应的是什么，借此机会深入理解svm，好更好的掌握这个工具的使用。然后还会再用同样的方法，实践体会其他的分类模型，比如逻辑斯蒂回归、神经网络等等。</p>
]]></content>
  </entry>
  
</feed>
