<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 文本挖掘 | Xutianming Blog]]></title>
  <link href="http://xutianming.github.io/blog/categories/wen-ben-wa-jue/atom.xml" rel="self"/>
  <link href="http://xutianming.github.io/"/>
  <updated>2016-08-31T23:02:44+08:00</updated>
  <id>http://xutianming.github.io/</id>
  <author>
    <name><![CDATA[Xutianming]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[行业关键词分类整理2]]></title>
    <link href="http://xutianming.github.io/blog/2013/11/14/text-classification-step-by-step2/"/>
    <updated>2013-11-14T12:45:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/11/14/text-classification-step-by-step2</id>
    <content type="html"><![CDATA[<p>在上一篇博客里，详细讲述了项目的具体过程。项目结束之后，从昨天开始，我就开始着手去查看一些资料，整理思路，看看项目的过程中，哪些是有理论依据、切实可行；哪些只是基于个人经验的推论，可能存在很多的纰漏。从而能找到一些提高的可能性，同时也梳理一下整体的思路，在以后遇到问题的时候，可以有的放矢。知道哪些地方可以不再进行尝试了，哪些地方需要着重考虑。</p>

<p>下面的思路整理，是参考了《信息检索导论》书中，对于文本分类的介绍，并且结合了这次项目的经历。书是2010年出版，偏向综述性质，也兼顾了一定的细节。毕竟只参考了一本书，不敢说尽善尽美，以后如果有读到更好的、更全面的，会再来补充的。</p>

<p>首先是文档的表示形式，有两种，一种是向量空间模型（VSM），目前我所使用的，全部都是利用向量空间模型来做的，即用特征向量来表示文档。第二种是，潜语义空间，是自然语言处理领域的，接下来，我会研究和尝试下这个方面，目前了解很少，就不多讲了。</p>

<p>下面的解决方案，我都是在VSM的基础上理解的，也许也能适用于潜语义空间。</p>

<p>总体来说，一个高准确率的分类系统，是由一个自动分类器和一系列人工撰写的布尔规则来组成的。前者由数据通过机器学习得来，后者由领域专家人工撰写。这一点，在我们的项目过程中，我大部分时间都是追求分类器的高准确率，基本上没有花很多时间去做规则。</p>

<p>在面对一个文本分类项目的时候，首先要关注的就是训练数据：</p>

<ol>
<li>假如没有标记好的训练数据，而有领域专家的话，采用人工编写规则的方法。这种方法虽然要花很多人力，但是往往分类的效果并不差，往往有高的准确率和召回率。</li>
<li>如果拥有的训练数据非常少。应该采用高偏差低方差的分类器，比如NB。而像KNN这种对训练数据拟合比较好的，低偏差高方差的分类器，就效果不太好。对应的如果我们使用SVM，参数C倾向于取的小一点。另外除了利用现有的训练数据外，通过半监督学习的方式，来增加训练数据，也是一个思路。主动学习系统，就是建立一个系统来确定需要标注那些文档。</li>
<li>如果拥有比较多的已经标注的数据（我们的这个项目就是这种情况）。那么分类器的选择，对于最终的效果就没有太大的影响。这种情况下，一般使用SVM，因为很多实验表明了，SVM是同等条件下来说，效果比较好的分类器。在我们的项目中，就有这种情况，我们尝试了NB和线性核的Liblinear，都没有本质的提升，准确率都在80%~90%之间。</li>
</ol>


<p>在上面的第三种情况下，分类器的选择并不重要，那么如果提高分类的效果呢？</p>

<ol>
<li>撰写人工规则。这对于修正某一特定类的误伤，比较立竿见影。指望这个，大幅度提高准确率，感觉还是要有很多工作要做的。</li>
<li>使用领域特征。一个普遍的事实，采用领域相关的文本特征，在性能上会比采用新的机器学习方法获得更大的提升。</li>
<li>采用复杂的分类体系。比如层次系统，比如使用弱分类器的组合，Voting、Bagging、Boosting等方法。再辅助以人工审核，将低分类置信度的数据，纳入审核系统。</li>
</ol>


<p>下面详细说一下，其中的第二条，也就是领域特征的选取。</p>

<p>对文本分类问题，如果对待特定的问题加入合适的额外特征，分类的效果还能显著提升。这一过程往往称为特征工程，目前来说，特征工程还是主要依赖人的技巧而不是机器学习的结果，好的特征工程往往可以大幅度提高分类器的效果。（写到这里我想，那些在项目中能有95%以上准确率的，应该都是花时间做了特征工程的。到时候公布冠军的代码的时候，可以验证一下。）</p>

<p>除此之外，还有一些技巧。</p>

<ol>
<li>将特殊字符串替换成更知名的词条。举例来说，在化工类中，可能出现氯化钠、氧化钙等名词，假如没有类似的类的话。这些应是非常有区分度的特征，但是在分类过程中，首先分词器可能无法识别这种词，其次用chi-square也许能提取出这种特征，但是在tf-idf赋权的时候，他们的权重也不可能高（因为不会出现太多次）。假如把这些词统一替换成，“化学物质”类似的更知名的词条，那么化学物质就能是个很好的特征了。</li>
<li>有时候，词的部分和词的多词模式，也许是很好的特征。这很类似于我们在项目采用的2-gram、3-gram组合词。总的思路是先通过好的方法，去发现一些好的多词模式的候选，加入特征集之中，再利用常规方法去选择特征。具体用什么方法去挖掘好的多词模式以及如何使用。就目前来说，对我都是一个疑问。接下来，我也会有针对的性的读一些这方面的资料。</li>
<li>基于文章结构。比如特征出现在标题中，特征的权重翻倍。新闻类文章，特征出现在第一段或者每段的第一句话中，增加权重之类的。这就是基于位置的特征选择方法。在之前的Wikipedia的项目中，有一些类似的情况。</li>
</ol>


<p>综上可知，大致可做的工作就这么些。基本上对于每个文本分类项目，都有自己的特性，也许整体思路都是一样的。但是各个环节也都影响着最终的结果。结合我们的这个行业关键词分类的项目来看，我们可做的工作有：</p>

<ol>
<li>撰写人工规则。这个虽然可以提升效果，获得好成绩，但是目前阶段来讲，我更倾向于直接从别人那里获得做这个工作的最佳实践而不是自己去花大量的时间去实践。所以这个方向暂时不考虑，可以读一些方法性的文章。</li>
<li>领域特征选择。如何去做特征工程？如何可以系统的去选择领域特征。这是个需要研究的方法性问题。</li>
<li>如何选择和使用多词模式。凭借目前对这份数据的浅薄的理解，我们提出了一些方法，但是实践证明效果不好。我们首先，组合出所有可能的2-gram，然后利用2-gram出现的频率，以及2-gram的组成单元出现次数的比例，来筛选2-gram。选出来之后，我们就去根据这些2-gram去合并了原始的中文分词结果。这样相当于我们可能会淹没掉2-gram的每个组成单元的，可能的高区分度。这种方式显然不好，所以需要通过更多的阅读，来获得这方面的确定性的知识。而不是自己胡乱尝试。</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[行业关键词分类项目总结1]]></title>
    <link href="http://xutianming.github.io/blog/2013/11/14/text-classification-step-by-step/"/>
    <updated>2013-11-14T10:24:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/11/14/text-classification-step-by-step</id>
    <content type="html"><![CDATA[<p>在过去的一个月里，完成了一个行业关键词分类的项目，100w的训练数据，1000w的测试数据，33个分类。项目期限已经到了，但是我们的准确率目前只有88%，尝试了很多方法，遇到很多困难，也走过弯路。</p>

<p>项目结束并且效果不好，自然要寻找原因。一方面是等待比赛完全结束，公布冠军的解决方法，来开阔下眼界；另一方面就是更广泛的读一些相关的资料，做一些在项目过程中没有精力去做的阅读和学习工作。</p>

<p>首先简单描述下项目的过程。这次项目提供了33个分类，和一些要分类的关键词。每个关键词，提供了10个搜索引擎的自然排序的结果，以及购买这些关键词的用户。一部分已经标注的训练数据，和大量要分类的测试数据，分类效果的评估用的是宏平均(Macro-average)准确率。接下来，我们做了如下尝试：</p>

<ol>
<li>利用提供的文本数据进行文本分类。其中包括了利用lucene的IKAnalyzer进行分词、利用Chi-square进行特征选择、利用tf-idf作为权重生成文本的向量表示形式、利用Liblinear进行线性核的分类。第一次准确率87%。由于以上工作都是在Hadoop上完成的，加上学习和适应Hadoop平台的时间，其实只要一天的工作量的事情，做了2周（比赛承办方提供的Hadoop集群的限制实在是太多了）。</li>
<li>利用一些人工制定的规则，挖掘一些有着高区分度的词，利用这些词去修正上述文本分类的结果。这一步，把准确率提高到了88.5%。这也是我们最好的成绩了，本来可以利用这种方法获得更多的准确率的提升的，但是感觉可学习的东西很多，不想把更多的时间放在对数据的理解上。</li>
<li>利用关键词的用户购买信息去做关键词分类。这样做是建立在“购买关键词的用户相似，那么关键词倾向于属于同一类”这一假设的基础上的。所以想对&lt;关键词-用户>向量，应用分类算法。这一矩阵存在着维度过高（600w维）和过于稀疏的问题（很少有两个用户买同一个关键字）。所以推断直接分类既不可行，效果也不好。</li>
<li>这一条详细讲一下用户矩阵的使用。首先想到的是对用户聚类，以降低&lt;关键词-用户>向量的维度和稀疏性，使用k-means。但是由于上面所说的稀疏性问题，所以聚类效果也不好。于是想到利用第一次文本分类的结果，补充训练数据，另外首先对关键词聚类（已经聚好了，33类），这样&lt;用户-关键词>向量维度33维，并且不稀疏。可以使用k-means对用户进行聚类了，分别制定k为100和1000，进行聚类。在使用的过程中，发现了一个问题，k-means一般是基于距离的，而在这个项目的用户中，和质心距离相等的两个点，并不一定相同，距离计算过程中把所有的维度（关键词类别）等同看待，而现实中，我们购买了不同维度（类别）的关键词的用户，虽然距离相等，但是是属于不同类的。基于上述思考，我放弃了使用k-means，按照自己的想法，对用户进行了聚类。得到了2w类左右的用户。用户聚类之后，就达到了对&lt;关键词-用户>矩阵的降维和去稀疏的目的。可以应用分类算法了。</li>
<li>利用上述的方法，利用用户对关键词分类。但是结果非常不理想。原因在于，我们在第3步里的假设是不对的。也就是说，假如几个关键词，被几个相同或者相似的用户购买，并不代表关键词属于同一类。分析bad case可以看出，比如公司转让，服装公司转让被分入了服装类，房地产公司转让被分入了房地产类。但是这些关键词应该是被一个公司转让中介性质的用户购买了。所以从用户角度去分类，是不可行的。</li>
<li>经过上面的尝试，时间已经过去了3周。我们放弃了使用用户购买信息，试图提高文本分类的效果。首先我们发现，需要利用多词模式去提高特征质量，比如，搬家是个特征词，公司不是，但是搬家公司，是个非常有区分度的特征。由于没有事先的积累，也找不到现成的工具，我们自己根据经验和对数据的理解，写了bi-gram和tri-gram的map-reduce组合算法。使用频率对多词模式进行筛选，然后加入特征集训练。这一步，问题很多，因为大部分是拍脑袋定的，感觉很不科学，接下来，我可能会对这方面做一些针对性的阅读。</li>
<li>重复了文本分类的步骤，做了第二版的分类器，但是由于多词模式的筛选和使用都有些问题，所以这一版分类器没有取得预期的进步。</li>
<li>时间到了最后一周，感觉也没有什么特别好的点子了。偶然之中发现了Multi-class 分类相关的知识，比如one-vs-rest，one-vs-one等等。以为发现了新的方法，于是我们用Hadoop又做了一个one-vs-rest的多类分布式分类器，主要的思路是参考的别人的，花了很多时间去读别人的代码以及了解liblinear的各项接口和参数。这段时间算是对SVM和Hadoop编程有了更进一步的知识。无奈，最终发现，其实本来Liblinear做多类分类，就是用的one-vs-rest。而且我们的分类器，最终效果也不好。</li>
</ol>


<p>这就是过去一个月中，我们进行的所有工作。学到了很多知识，对SVM，NB分类器，对Hadoop的java接口和Streaming以及map-reduce的思想都有了进一步的认识，并且可以顺利使用了。虽然最终结果并不好，但是对于缺乏经验的我们来说，算是一个很好的经历。下篇文章里，我再结合一些书本上的知识，讲一些关于这个项目的思考，和可能的改进方向。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[知识可视化项目第一阶段小结]]></title>
    <link href="http://xutianming.github.io/blog/2013/09/23/summary-of-knowledge-visualization-project-stage-1/"/>
    <updated>2013-09-23T14:44:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/09/23/summary-of-knowledge-visualization-project-stage-1</id>
    <content type="html"><![CDATA[<p>距离开始做数据可视化项目，已经过去了将近一个月的时间，目前项目的进展暂时告一段落，技术探索的工作基本完成，是个时候小结一下了。</p>

<p>目前项目的思路是这样的，以维基百科为数据源，将维基百科上的知识（目前以机器学习为例）用可视化的方式展现，提高学习和探索的效率。</p>

<p>具体来说，第一步。编写网页爬虫，从维基百科上下载数据，并且存储成结构化的数据（目前采用XML），存储的内容包括分类、关键词定义、页面关键词、参考文献。其中，关键词的定义，我们认为是维基百科词条页面的第一段，实际效果还不错，第一段文字往往能很好的直接阐释关键字的含义。页面关键词，就是那些本页面中比较重要的词，要想很好的了解本页面的内容，这些关键词对应的内容是非常重要的参考资料，在实现中，我们以页面内容中超链接的锚文字为基础，筛选那些链接指向的页面也包含本页关键字的链接锚文字，这一点是受到PageRank的思想所启发的；就效果来看，关键字筛选效果良好，但是这个条件明显太强了，在两个条件的限制下，大部分页面都只有7个以下的关键字，这样使得可视化的意义不大了，因为内容本来不多的情况下，单纯的文本就很容易理解的。参考文献，就是单纯是指维基百科词条页面中的参考文献，在目前的实现中，这些数据并没有加以利用。至于这些内容是如何从Web页面中提取的，主要靠对页面结构的分析，编写在网页爬虫的逻辑中。</p>

<p>第二步，是文本挖掘相关。针对上面的页面关键词，我们想要给每个页面关键词，根据相关度进行打分，然后根据这个相关度打分，进行进一步的可视化，比如比较重要的关键词Size比较大什么的。具体研究，使用了NLP中称为关键字提取（Keyword Extraction）的技术，通过阅读相关论文，确定了卡方相关性检验和仿PageRank的TextRank算法这两种技术方案。二选其一的话，我们选择了卡方检验，因为TextRank对我们来说有一些致命的缺点，比如计算量大、基于词在滑动窗口中的同时出现等等。相对来说，卡方检验对我们来说，要现实的多。</p>

<p>在我们参考的论文中，作者针对单篇论文，对关键字进行提取，打分自然就是卡方值。对语料数量的要求都不是特别高，但是对我们来说，我们现有的语料还是太少了，维基百科一般的词条页面中内容很少，用这些来进行关键词提取的话，估计效果甚微。于是我们决定扩充语料，经过研究，我们决定利用机器学习方面的会议(NIPS)及其提供的搜索引擎。因为项目做到这里我们已经决定先就一个词条，进行挖掘，实验下看下效果，所以就选了Machine_learning这个词条。扩充语料的过程还是用爬虫来完成的，模拟执行搜索引擎搜索，爬10篇论文下来作为语料。期间需要把pdf格式的转化成文本。经过一些折磨好歹完成语料的准备之后，开始进行关键词提取了，具体过程，一开始，我们想就第一步率先选出的关键字进行打分，算卡方值。但是语料准备好之后发现，我们选出的那些关键字，在这10篇论文中出现很少。究其原因，主要是我们扩充语料的思路有问题，试想维基百科中的词条，应该是比较宽泛的介绍性文字，而论文多是针对领域中某个小的问题的。从维基百科中挖掘出的关键词，很难应用到论文的语料中来。所以只能再次委曲求全，放弃对维基百科的关键字进行打分，改为直接从论文语料中进行关键词提取。</p>

<p>接下来的过程就是按照论文中的方法一步一步来了，涉及到词干提取，词组提取，词组聚类，计算卡方值等等。也顺利提取出了关键字，效果还不错。这么一来数据挖掘和准备的工作基本上就完成了。</p>

<p>第三步，可视化。可视化框架选择了D3js。用树状来表现分类，节点作为词条，点击词条，可视化展示此词条对应的关键词，使用了BubbleChart，主要涉及了D3js相关的javascript开发。</p>

<p>截止今天，以上的工作就是第一阶段做的事情。总的来说，没有成功的基于维基百科的数据进行挖掘，但是也探索了一条可行的路线，只是这条路线看样子扩展性并不好。于是，近几天忙于寻找一些更好的方式，不论是从数据源还是从挖掘的效果上。最近发现了一个专业的维基百科挖掘工具，(WikipediaMiner)[<a href="http://wikipedia-miner.cms.waikato.ac.nz/index.html">http://wikipedia-miner.cms.waikato.ac.nz/index.html</a>]和weka出自同一所大学，<a href="http://sepans.com/wikistalker/">效果</a>非常好。所以下一步，决定针对这个进行一些研究。先研究其原理，对比下我们挖掘的算法，看看差距在哪里，然后试图利用这个现有的工具，做一些工作。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[卡方校验、信息增益和互信息]]></title>
    <link href="http://xutianming.github.io/blog/2013/09/14/chi-square-information-gain-and-mutual-information/"/>
    <updated>2013-09-14T20:51:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/09/14/chi-square-information-gain-and-mutual-information</id>
    <content type="html"><![CDATA[<p>从一开始做文本分类到现在做的项目，一直在不断的学习新的知识，练习已经掌握的工具，也越来越深刻的认识到，“你所掌握的机器学习方法，并不是死板的一成不变的算法，而是一个一个的小工具，是解决问题的思路”。</p>

<p>这篇博客将对比三个在我进行文本挖掘相关工作的时候经常用到的小工具，以及我对他们的认识，仅限个人的粗浅观点，也是我当前的认识，不见得正确。</p>

<p><a href="http://zh.wikipedia.org/wiki/%E7%9A%AE%E7%88%BE%E6%A3%AE%E5%8D%A1%E6%96%B9%E6%AA%A2%E5%AE%9A">卡方校验</a></p>

<p>首先理解卡方校验的概念，除了维基百科，我总会看<a href="http://www.blogjava.net/zhenandaci/archive/2008/08/31/225966.html">这篇博客</a>。简单来讲，卡方校验是用来判别两个事件是否相关的方法，因此在特征提取、关键词提取中都有很多的应用。基本思路是，首先假设A、B不相关，然后根据A和B的联合概率分布，计算卡方值，公式在定义中都有给出，卡方值越大，越倾向于推翻原假设，也就是A、B越相关。在计算卡方值的过程中，最关键的问题是找到联合概率分布。那么我们应该如何理解卡方值呢？卡方值的含义其实是这样的，我们首先假设A、B不相关，也就是AB相互独立，那么我们可以为联合概率分布中的AB同时成立的事件计算期望值，根据定义公式显示，卡方值是这个期望值和实际值之间的差的平方除以期望值的和（定义和方差很像！），衡量了这两个量拟合的程度。那么，卡方值越大，代表期望值和实际值之间的差越大，也就是我们计算期望值所依赖的假设，是不成立的！</p>

<p>当我们想用卡方检验的方法，去判断两个事件是否相关的时候，理解问题的过程，就是建立事件A、B联合概率分布的过程，找到这个概率分布，就可以计算卡方值。举例说明的话，在文本分类的特征提取这一步，事件A是特征词w是否存在在本篇文档中，事件B是本篇文档是否属于类c。首先假设“w不是类c的特征词”，那么词w在属于类c的所有文档中出现的概率，应当与w在全部文档中出现的概率相同，我们依据这一假设，可以算出w出现在c类的文档数的期望，用期望值和实际值的差的平方除以作为卡方值。</p>

<p><a href="http://zh.wikipedia.org/wiki/%E7%9B%B8%E5%AF%B9%E7%86%B5">信息增益</a></p>

<p>也经常作为特征提取的方法，也用来衡量两个事件的相关性，但是是从信息论的角度来的。信息论用熵来衡量信息量。信息增益，简单理解就是在已知条件B的情况下，事件A的信息量的改变程度，就是熵的改变程度。同样有<a href="http://www.blogjava.net/zhenandaci/archive/2009/03/24/261701.html">一篇博客</a>讲得很透彻。从上面的描述可以看出，计算信息增益的关键是计算熵和条件熵。所以说，信息增益对应的概率分布，是普通的概率分布和一个条件概率分布。那么我们应该如何具体理解信息增益呢？我们首先计算事件A的信息熵，然后考虑事件B，计算在有事件B的情况下的条件熵，两者之差，就是信息熵的改变，改变越大，说明事件B对于事件A的相关性越大。由于涉及到信息论，所以信息量改变这一点，就变的略显抽象。</p>

<p>仿照上文中对卡方校验的介绍。利用信息增益解决问题的关键，在于找到这个条件概率分布，以计算条件熵。同样举文本分类中特征提取这个例子。事件A是指文档属于类ci，事件B是词w是否出现在文档中。根据P(Ci|w)和P(Ci|^w)这两个条件概率分布，可以计算条件熵H(C|T)。</p>

<p>不过仔细读过信息增益的概念，发现我们上面介绍的和KL散度差别挺大的，目前我也不能理解这两种概念是否本质上相同，但是在分类和挖掘领域，大家一般是像我说的那样去理解信息增益的。</p>

<p><a href="http://zh.wikipedia.org/wiki/%E4%BA%92%E4%BF%A1%E6%81%AF">互信息</a></p>

<p>在互信息定义的多条公式中，我一般使用第三个，就是X，Y熵之和减去联合熵。根据联合熵，就可以发现，决定互信息的，也是一个联合概率分布。</p>

<p>比较形象的理解互信息的公式方法是，认为事件X和事件Y相关，可以让总信息熵减少多少，差别越大，X和Y越相关。计算互信息的关键，是计算联合熵，因此在用互信息解决问题的适合，找到这个联合概率分布，是核心。不要试图去寻找其他的概率分布，实践表明，互信息的联合概率分布与卡方检验的非常相似，都是x是否怎样，y是否怎样。同样以文本分类为例，X为文档是否属于类c，Y事件为文档是否包含w，然后利用这个概率分布去计算联合熵。这么一看，和卡方检验几乎完全相同！只是利用了不同的理论基础。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[文本分类实践]]></title>
    <link href="http://xutianming.github.io/blog/2013/07/19/text-classification-in-action/"/>
    <updated>2013-07-19T22:14:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/07/19/text-classification-in-action</id>
    <content type="html"><![CDATA[<p>学习机器学习有一阵子了，先是仔细阅读了《统计学习方法》这本书，对目前比较流行的统计学习方法和模型有了大致的认识，比如朴素贝叶斯、逻辑斯蒂回归、支持向量机、隐马尔科夫模型等等。但是仅仅有理论上的认识是不够的，关键还是指导实践解决问题。</p>

<p>先前在百度实习的时候，已经对机器学习有了初步的认识，并且当时也利用逻辑斯蒂回归训练了一个还算不错的模型，虽然最终由于时间原因，没有做成一个可以应用到线上的模块，但是对于我对机器学习的这第一次尝试，我还是很满意的，当时也决定在研二的这一年里，好好研究一下机器学习相关的东西，以后好找相关方向的工作。</p>

<p>最初的实践决定从文本分类开始，因为这个领域相对来说研究的比较成熟，并且在工业界仍然有很多很重要的应用。由于svm一直是个非常重要的机器学习模型，之前对原理有一定的了解，于是决定从使用svm开始。网上相关的教程很多，我也是慢慢参考着别人的方法一步一步尝试。这里主要记录下我实践的过程，以供日后参考。</p>

<p>文本分类大致分为以下几步：</p>

<ul>
<li>数据预处理（一般是分词，去停）</li>
<li>特征选择（比较好的方法有卡方校验和信息增益）</li>
<li>特征值确定（比较好的是tf-idf值）</li>
<li>特征向量计算</li>
<li>模型训练</li>
<li>模型测试</li>
</ul>


<p>我使用的是复旦大学自然语言理解实验室提供的预料，选取了computer、agriculture、sports和history，选择分类的时候特意选择了差别比较大的几类，这样可以在某种程度上提高分类效果吧。</p>

<p>首先是数据预处理，我使用了同事推荐了一个分词工具，分词的效果不是很好，算是个初级的分词工具，基于python的，使用起来非常简单。分词之后，把预料中的文章，处理成了一个词一行的文档，便于下一步的处理。</p>

<p>数据预处理之后，就是选择特征了，这里所谓的“特征”，就是上一步分出来的一个一个的词，每个词都可以说是一个特征，但是把每个特征都哪来使用，显然是不现实的，太高维的特征向量，首先训练起来比较费时，另外会使模型复杂度提升从而容易使svm训练过拟合。我训练的时候，本想使用1w维的特征，最终使用了7600左右的特征，具体的我后面会说。考虑到以上原因，所以要有个特征选择的过程，也就是选择那些可以很好的区分每一类的词，比如包含了“农业”一词的文章，属于农业类的可能性比较大，所以这样的词，就算是好特征。那么如何筛选出这些特征呢，要有个数学上的量，来衡量特征的好坏，可供选择的有互信息、信息增益、卡方校验等方法。其中互信息的效果比较差，我就没自己尝试，我先后使用了信息增益和卡方校验的方法。</p>

<p>卡方校验，学名是“皮尔森卡方检验”，是一种假设检验方法。首先，我们的假设是“词word和类class不相关”，然后针对包含word的文档和以及属于class的文档的联合分布，算卡方值，每类选择卡方值最大的2500个值，去作为特征值，因为卡方值越大，倾向于原假设为假，也就是说，卡方值越大，词word和类class越相关，这个词就是我们要找的好特征。为啥是2500个值，因为我初步决定选择1w个特征么，一共四类，每一类选择2500个。最终计算下来，因为分词效果的原因，很多单字的特征被筛选出来，但是单字的信息非常少，我觉得不足以作为一个好的特征，所以我对于筛选出来的1w个特征，去除了那些单字的词，就剩下了大约7600个词作为特征。</p>

<p>特征选择出来之后，我们要利用这些特征去表示要分类的文档，也就是所谓的“把文档映射到特征空间”，用向量来表示文档。这里会遇到一个问题，如何用特征表示文档，另外每个特征对于一个文档都是等同重要的嘛？这里就自然涉及到为特征赋权重。我是用的是tf-idf值。选择这个的原因是，卡方校验有个缺陷，就是“低频率缺陷”，那些在一篇文档里只出现过一次的词，也有可能会有很大的卡方值，但是这个词，对这篇文章可能没太大价值，那些只在某一类文章中大量出现，而在其他类中很少出现的词，才是决定类别的词。而我个人认为tf-idf值可以比较好的弥补这一缺陷。当然我只是定性分析，我不太懂数学上的证明，也没去查阅相关的资料。</p>

<p>如此一来，我们把文档用特征向量来表示了，接下来就是训练模型了。在上述过程的实现过程中，我没有遇到太大的问题，unix+python是很有利的文本处理工具，实现起来很快。我是用的svm工具是libsvm，使用也很简单，但是我确实在训练模型的时候遇到了一个问题。</p>

<p>一般来讲，libsvm的使用流程是这样的：</p>

<ul>
<li>svm-scale，对数据进行归一化处理，映射到[0,1]区间内</li>
<li>grid.py 寻找最优参数c和g</li>
<li>svm-train</li>
<li>svm-predict</li>
</ul>


<p>我第一次使用的时候，用信息增益选择特征，libsvm不太懂，也没去查太多的资料，略去了第二步，也就是我训练的时候没指定参数。这训练效果太差了，把所有的文档都分到了一类中。一开始我总怀疑特征提取的不好，于是换卡方校验，结果还是这样，之后开始怀疑我训练的方法不对，于是很快发现了问题。在指定c和g参数之后，训练结果非常好，对训练数据的准确率为100%，在测试集上的准确率为93.52%，比人家论文上的准确率还高，非常让人振奋。</p>

<p>至此，算是完满的完成了一次svm文本分类的实践，结果也符合之前给自己定的目标90%以上的准确率。接下来，我会根据这次实践的结果，总结分析，再回归到svm的理论上，比如c和g具体含义对应的是什么，借此机会深入理解svm，好更好的掌握这个工具的使用。然后还会再用同样的方法，实践体会其他的分类模型，比如逻辑斯蒂回归、神经网络等等。</p>
]]></content>
  </entry>
  
</feed>
