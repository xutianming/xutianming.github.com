<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Xutianming Blog]]></title>
  <link href="http://xutianming.github.io/atom.xml" rel="self"/>
  <link href="http://xutianming.github.io/"/>
  <updated>2014-08-02T17:05:35+08:00</updated>
  <id>http://xutianming.github.io/</id>
  <author>
    <name><![CDATA[Xutianming]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[对算法、工程和个人方向的一些小思考]]></title>
    <link href="http://xutianming.github.io/blog/2014/08/02/a-brief-summary-of-internship-in-alibaba/"/>
    <updated>2014-08-02T15:48:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2014/08/02/a-brief-summary-of-internship-in-alibaba</id>
    <content type="html"><![CDATA[<p>从5月份从搜狐离职进入阿里实习，已经有几个月没写博客了。这也基本上反映了我这段时间的状态，实习、个人学习，占据了这段时间我的所有时间。一直拼命奔跑，很久没停下来好好思考总结下了。校招马上开始，陆续会有很多面试，也是时候对自己的这段时间进行下回顾和思考了。</p>

<p>去年底来到搜狐，和克竞的合作，让我慢慢真正走入了算法的大门，让我对机器学习的理论有了一些比较深刻的认识，当然仅仅限于算法本身，我的工作，并没有涉及太多的业务。我也做了不少和算法工程实现相关的工作，也接触到了Spark。克竞离职之后，很多事情青黄不接，我的工作也一直没有太大进展，考虑到时间的宝贵，我离开搜狐来到了阿里，投奔钩哥。说是“投奔”钩哥，确实如此。我抱着可以对机器学习有更加深入的理解的目的，来到新的工作环境，开始了我新的旅行。如果说，在搜狐我真正接触了算法，那么在阿里，我对业界的算法有了更加真实的认识和一些理性的思考。</p>

<p>阿里是一家相当务实的公司，所有的工作都是以业务发展为导向的。我所在的爱淘宝，目前的算法工作，主要是以策略为主，大多用传统的数据挖掘方法，机器学习的模型尚未大量使用。这一点，和我在搜狐研究院的时候的认识，有着比较大的出入，当时团队在工作的时候，更偏向于使用模型来解决问题。这是两种做事的风格，Leader作出这些选择，都是从团队的位置和目的出发的，无法评价说孰优孰劣。只能说，从对业务的发展来看，阿里更加务实，也会更加快速的迭代，业务发展更快。</p>

<p>如此来说，短短几个月的实习时间里，我很难接触到模型相关的工作，而我个人并不是特别喜欢过早是深入某个业务领域。而各种数据权限限制，也让我这个实习生很难完全接触到阿里丰富的数据。我认识到这些，大约花了一个月的时间。接下来，我作出了转变。</p>

<p>我在阿里的师兄是大家认可的团队里的技术大牛。我申请做了一些C++线上服务开发，也就是偏工程的工作。之前C++的学习和积累得到了发挥，我的工作也得到了师兄的认可，第一个项目马上就要上线，紧接着就要开始第二个项目了。</p>

<p>这些经历让我慢慢发现了自己更加喜欢、也可能对我来说最现实的工作：那就是转而做系统。</p>

<p>首先，做算法，对我来说，并不现实。技术要为业务服务，算法也不例外。要想成功的完成一个算法项目，特别是利用机器学习的方法。需要两方面：对数据的深刻理解和对模型的深刻理解。两者缺一不可。盲目的使用模型，不会带来效果的提升，消耗大量工作力而没有效果，在公司中对自己非常不利。那么更加务实的方法，就是先去理解数据，从分析业务，先解决关键问题，脚踏实地，一步一步优化效果。对数据理解足够深刻之后，算法工程师的模型方面的知识才会派上用场。前者需要时间沉淀，后者需要深厚的理论知识背景。总得来说，我两方面都没有，更关键的是，专注业务会让我越来越远离自己想成为的人。</p>

<p>然后，一直以来，我都追求技术上的成就，希望成为一名出色的工程师。更加现实的，想要在互联网行业里有比较好的发展，技术是立足之本。繁重的业务需求，会让人停留于表面，技术上毫无成长，更好的方式显然是在业务成长的同时获得个人技术沉淀。但是，这样共同成长现实么？不得不说，大部分人做不好。目前国内互联网的发展思路是“快”，凡是讲究快速上线、快速迭代，业务上无创新、无改变，意味着离死也不远了（公司核心营收大头除外，比如搜索广告）。在这个背景下，大部分团队，都是一招鲜吃遍天，不断的使用现有框架进行业务扩展。技术上的纵深发展很少。</p>

<p>那么个人的成长，如何来实现呢？靠团队、靠自己。在更好的技术团队工作，在业务线上工作的同时，注重个人的技术积累。就目前我的认识，总结来说，分布式、并发、高性能、网络编程都是行业中非常核心的技术，个人纵深的发展，一定要落入某个桶中。而语言和算法，是核心基础。团队也非常重要，但是由于信息的高度不对称，找到一个靠谱的团队并且能顺利进入，太难了。</p>

<p>不得不说，这一点上，淘宝的正明做得简直太好了。一直利用自己在技术上的深入研究，为阿里开源节流，和公司共同成长，让人佩服不已。</p>

<p>综上来看，做系统、平台开发，做工程，对我来说是一个相对来说比较现实的选择。特别是机器学习算法相关的工程实现的话，便既可以让我继续研究和学习算法又可以与业务保持一定的距离，更专注于技术本身。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[分布式机器学习算法框架的设计与实现]]></title>
    <link href="http://xutianming.github.io/blog/2014/03/02/hadoop-owlqn/"/>
    <updated>2014-03-02T14:52:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2014/03/02/hadoop-owlqn</id>
    <content type="html"><![CDATA[<p>最近一直在研究逻辑斯蒂回归算法。参考的是MSRA的Galen Andrew以及Jianfeng Gao的OWLQN算法。这一算法，是基于L-BFGS算法提出的。目标函数是带L1规范化的L2逻辑斯蒂回归损失函数，也就是说，即有L1也有L2，也可以只取其一。算法中，为了解决带L1规范化的目标函数在0点不可导的困难，定义了伪梯度。其他的步骤，与经典的L-BFGS算法大体一致。</p>

<p>这里给出几个参考链接，对于阅读论文有很大的帮助。</p>

<p><a href="http://en.wikipedia.org/wiki/Limited-memory_BFGS">L-BFGS</a></p>

<p><a href="http://en.wikipedia.org/wiki/Backtracking_line_search">Backtracking Line Search</a></p>

<p><a href="http://en.wikipedia.org/wiki/Wolfe_conditions">Wolfe Conditions</a></p>

<p><a href="http://en.wikipedia.org/wiki/Directional_derivative">Directional Derivative</a></p>

<p><a href="http://pan.baidu.com/disk/home">zjuOptimization课件</a></p>

<p>结合论文作者给出的单机LR的C++代码，可以比较快速的了解算法。但是我个人感觉原本的设计并不是特别好，C++的实现也比较一般，类的设计略显牵强。当然这是我从一个学习者的角度给出的评价，我代码读起来还很费劲的。</p>

<p>有了上述的知识以及对单机代码的掌握之后，我决定实现分布式的版本。基于Hadoop的MapReduce。首先分析，当数据量增大，所有的训练数据不足以存放在单机内存中时，算法的主要瓶颈在哪里。通过阅读代码可知，算法最耗时地方在于计算当前w下，所有训练样本的Loss的和，并更新梯度，也就是源码中的Eval函数。很自然的想法是，把数据分块，在每个Mapper上计算本样本的Loss和本样本对梯度更新的影响，在Reducer中对这些计算结果求和。这样，在LineSearch中，要进行多次的Eval，也就是每次迭代可能都要进行多次MR任务。这样的设计思路非常简单，但是存在很多限制，我会在博客的最后一一说明，这些改进将是下一步的工作方向。</p>

<p>下面主要说说我的设计。我说到单机C++版本的OWLQN设计的并不太好，我在我的设计中做了改进，也用了很多Java面向对象的特性。</p>

<ol>
<li>DifferentiableFunction接口。目标函数均为可导函数，所以当要扩展其他的算法时，只要实现此接口，本接口定义的功能有：函数求值、求某一点的梯度、求某一点在某方向的方向导数。</li>
<li>OptimizationSolution接口。定义最优化解法，本项目中为OWLQN，提供求解最优化的Minimize算法。当要扩展其他的解法时，比如梯度下降法，只要实现本接口就行。</li>
<li>IterationState抽象类。用于记录迭代中的信息，抽象类定义了一些基本的操作。</li>
<li>TerminationCriterion接口。迭代收敛的条件，用本条件和Tolerant做比较，小于Tol即为收敛。</li>
</ol>


<p>项目不大，但是Java面向对象的特性几乎都用到了。封装、继承、多态，面向接口和抽象类编程，即可在运行时根据传入参数的具体类型，调用对应的方法，非常灵活。</p>

<p>关于接口和抽象类，有很多相似点也有很多不同。它们都可以用来设计编程接口，让项目的扩展按照设计者的思路来，从语法层面限制扩展的方式。不同在于，接口更加灵活，因为Java类可以实现多个接口，但是只能继承一个类。类通过继承，有更强的限制性，并且使用继承的话，可以提供一些基本操作的实现，比如IterationState中的IterIncrement，增加迭代计数。</p>

<p>关于设计的改进，也就是我的设计与原项目不同在哪里呢？除了面向对象特性的应用外，主要的区别在于IterationState，原先的项目，把所有的计算，放在这个类里，并设置OWLQN类为FriendClass，非常的不合理，破坏了封装，逻辑上的划分也非常混乱。于是我在我的实现中，重新进行的划分和封装。</p>

<p>总的来说，新的设计，思路更加清晰并且扩展性也非常好。下一步，我计划将SVM也放入这个框架。</p>

<p>下一步的工作，主要是从本项目现有的缺陷出发。</p>

<p>本项目是基于Hadoop MapReduce的，有个非常大的限制，就是结点的IO。比较理想的情况下，我在进行每次迭代计算的时候，分到每个结点的数据集是不变的，变化的是当前的点X，那么这时候如果每个结点可以把分来的数据载入内存，每次迭代只根据不同的X进行计算，就可以避免本节点的磁盘IO，进而提高程序性能。但是MR框架，是不能避免磁盘IO的，每次都要读一遍数据。业界也早已发觉MR在分布式机器学习上的这一缺陷，于是有的基于MPI自己实现分布式，近两年也出现了新兴的开源的分布式计算框架Spark。这就是业界的方向了，我也会尽量的去了解下这些新兴的技术。</p>

<p>另外，在这个项目中，我设计的分布式算法还是很初级的，我的测试数据的维度并不高，那么我的训练数据矩阵按行处理，在和其他同学交流的时候，他们有提出，每一个训练sample也可以被划分，有可能会利用复杂的矩阵运算，来使得高维数据可以利用MR分布式计算。当然这只是一个初步的想法，具体的方向，还有待于进一步的阅读研究和请教。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[模型学习小结]]></title>
    <link href="http://xutianming.github.io/blog/2014/03/01/models-in-perspective-of-math/"/>
    <updated>2014-03-01T22:16:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2014/03/01/models-in-perspective-of-math</id>
    <content type="html"><![CDATA[<p>近来我对机器学习的模型有了一些更接近本质的认识。也让机器学习的这些理论，在我的眼里，变得不那么神秘。</p>

<p>机器学习的理论，可以分为两个部分，一部分是建模，一部分是求解模型。所有的机器学习算法，逻辑斯蒂回归、支持向量机，本质上来讲，都是一套建模的方法和一套可行有效的解法。</p>

<p>建模，从数学的角度，是对实际问题的描述。比如逻辑斯蒂回归利用逻辑斯蒂分布的特殊性质建模，也就是odds ratio为线性函数。而odds ratio是本sample为正例和负例的概率的比值的log。这样odds ratio的正负，可以代表本sample为正例的概率大还是为负例的概率大。而odds ratio的正负，就是线性函数的正负。因此有了逻辑斯蒂回归。建模的任务是描述问题，并得出目标函数，也即是Loss Function。Loss Function经常利用极大似然估计（Max likelyhood）和最小平方误差（Least squares）写出。支持向量机也是如此，支持向量机的建模过程中，认为sample到分类超平面的距离，衡量分类的置信程度。那么定义符合要求的分类超平面为，数据集中所有的点，到本分类超平面的距离，都大于等于R。然后极大化R，得到目标函数。</p>

<p>得到目标函数之后，就要求目标函数的极值。数学里的最优化问题，有非常多的方法。常见的有梯度下降法、牛顿法、拟牛顿法。理论上来讲，求解和建模是独立和分开的，也就是说，上面建模得到的目标函数，可以用任意一种最优化解法来解。当然这只是最理想的情况。实践中，每一种模型（目标函数），几乎都对应了一种实际可行的，可计算的解法。</p>

<p>从这种比较高层次的角度，再去读那些机器学习的书籍，会更加从容，应该可以把握的更好。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Java值传递和引用传递]]></title>
    <link href="http://xutianming.github.io/blog/2014/03/01/java-reference-and-value-parameter/"/>
    <updated>2014-03-01T21:25:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2014/03/01/java-reference-and-value-parameter</id>
    <content type="html"><![CDATA[<p>网上一搜Java引用传递，会出来很多博客，会举出好多例子来说明Java的引用传递。当然有一小部分博客说，Java是没有引用传递的。最近在做项目的过程中，我也遇到了这些问题，看这些中文博客更是一头雾水。只有经过了自己的实践，才有了可以让自己信服的结论：</p>

<p>Java里没有在C++意义上的那种引用传递，全是值传递。只是在Java中，对象分为基本类型和引用类型。典型的基本类型，包含所有的基本类型（primitive）；而对象等高级类型，属于引用类型。基本类型作为参数传递的时候，就是普通意义上的值传递，没啥争论，而引用类型作为参数的时候，传递的是本对象的引用的值。所以一概是值传递，而没有C++意义上的引用传递。下面也用代码来说明。</p>

<pre><code>public void setA(ArrayList&lt;Double&gt; a) {
    ArrayList&lt;Double&gt; b = new ArrayList&lt;Double&gt;();
    b.add(2.0);
    a.add(4.0);
    a=new ArrayList&lt;Double&gt;(b);
    a.add(3.0);
}
</code></pre>

<p>上面这段代码，运行之后，a里面只多了4.0。也就是说，只有<code>a.add(4.0)</code>起了作用。按照我们上面的理论，a是原数组的一个引用，指向原数组。那么<code>a.add(4.0)</code>自然可以影响到原数组。下面的<code>new ArrayList</code>语句，改变了a的指向，a指向一个新的数组。这时候再<code>a.add(3.0)</code>就是影响的新建的这个数组了。函数调用结束，a被还原指向原来的数组（这就是值传递的机制，原来数组的引用在函数调用之前保存在堆栈中，复制一份作为函数参数传递进去，函数调用结束恢复）。所以运行之后，a里只有4.0。</p>

<p>那么什么是引用传递呢。其实这是C++里的概念，就是传递的是引用。同样的代码用C++写。</p>

<pre><code>void setA(vector&lt;double&gt; a) {
    a.push_back(1.0);
    vector&lt;double&gt; b;
    b.add(2.0);
    a=b;
}
int main() {
    vector&lt;double&gt; a;
    setA(a);
    cout&lt;&lt;a.size()&lt;&lt;endl;
    return 0;
}
</code></pre>

<p>在这段代码里不论setA里做什么，a都不变。因为是值传递。那么C++的引用传递是什么呢？</p>

<pre><code>void setA(vector&lt;double&gt; &amp;a) {
    a.push_back(1.0);
    vector&lt;double&gt; b;
    a=b;
}

int main() {
    vector&lt;double&gt; a;
    a.add(1.0);
    setA(a);
    cout&lt;&lt;a.size()&lt;&lt;endl;
    return 0;
}
</code></pre>

<p>这样就能改变a的值了。</p>

<p>最后附上我所说的结论的证明。</p>

<p><a href="http://stackoverflow.com/questions/40480/is-java-pass-by-reference">http://stackoverflow.com/questions/40480/is-java-pass-by-reference</a></p>

<p>Java中的reference type 和 C++中的reference type是指的不同的东西的，在Java中，引用类型指的是Java对某一系列类型的处理方式，而在C++中，引用类型是一种切实的类型，和指针一样，可以与其他类型组合出一些复合类型。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[行业关键词分类整理2]]></title>
    <link href="http://xutianming.github.io/blog/2013/11/14/text-classification-step-by-step2/"/>
    <updated>2013-11-14T12:45:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/11/14/text-classification-step-by-step2</id>
    <content type="html"><![CDATA[<p>在上一篇博客里，详细讲述了项目的具体过程。项目结束之后，从昨天开始，我就开始着手去查看一些资料，整理思路，看看项目的过程中，哪些是有理论依据、切实可行；哪些只是基于个人经验的推论，可能存在很多的纰漏。从而能找到一些提高的可能性，同时也梳理一下整体的思路，在以后遇到问题的时候，可以有的放矢。知道哪些地方可以不再进行尝试了，哪些地方需要着重考虑。</p>

<p>下面的思路整理，是参考了《信息检索导论》书中，对于文本分类的介绍，并且结合了这次项目的经历。书是2010年出版，偏向综述性质，也兼顾了一定的细节。毕竟只参考了一本书，不敢说尽善尽美，以后如果有读到更好的、更全面的，会再来补充的。</p>

<p>首先是文档的表示形式，有两种，一种是向量空间模型（VSM），目前我所使用的，全部都是利用向量空间模型来做的，即用特征向量来表示文档。第二种是，潜语义空间，是自然语言处理领域的，接下来，我会研究和尝试下这个方面，目前了解很少，就不多讲了。</p>

<p>下面的解决方案，我都是在VSM的基础上理解的，也许也能适用于潜语义空间。</p>

<p>总体来说，一个高准确率的分类系统，是由一个自动分类器和一系列人工撰写的布尔规则来组成的。前者由数据通过机器学习得来，后者由领域专家人工撰写。这一点，在我们的项目过程中，我大部分时间都是追求分类器的高准确率，基本上没有花很多时间去做规则。</p>

<p>在面对一个文本分类项目的时候，首先要关注的就是训练数据：</p>

<ol>
<li>假如没有标记好的训练数据，而有领域专家的话，采用人工编写规则的方法。这种方法虽然要花很多人力，但是往往分类的效果并不差，往往有高的准确率和召回率。</li>
<li>如果拥有的训练数据非常少。应该采用高偏差低方差的分类器，比如NB。而像KNN这种对训练数据拟合比较好的，低偏差高方差的分类器，就效果不太好。对应的如果我们使用SVM，参数C倾向于取的小一点。另外除了利用现有的训练数据外，通过半监督学习的方式，来增加训练数据，也是一个思路。主动学习系统，就是建立一个系统来确定需要标注那些文档。</li>
<li>如果拥有比较多的已经标注的数据（我们的这个项目就是这种情况）。那么分类器的选择，对于最终的效果就没有太大的影响。这种情况下，一般使用SVM，因为很多实验表明了，SVM是同等条件下来说，效果比较好的分类器。在我们的项目中，就有这种情况，我们尝试了NB和线性核的Liblinear，都没有本质的提升，准确率都在80%~90%之间。</li>
</ol>


<p>在上面的第三种情况下，分类器的选择并不重要，那么如果提高分类的效果呢？</p>

<ol>
<li>撰写人工规则。这对于修正某一特定类的误伤，比较立竿见影。指望这个，大幅度提高准确率，感觉还是要有很多工作要做的。</li>
<li>使用领域特征。一个普遍的事实，采用领域相关的文本特征，在性能上会比采用新的机器学习方法获得更大的提升。</li>
<li>采用复杂的分类体系。比如层次系统，比如使用弱分类器的组合，Voting、Bagging、Boosting等方法。再辅助以人工审核，将低分类置信度的数据，纳入审核系统。</li>
</ol>


<p>下面详细说一下，其中的第二条，也就是领域特征的选取。</p>

<p>对文本分类问题，如果对待特定的问题加入合适的额外特征，分类的效果还能显著提升。这一过程往往称为特征工程，目前来说，特征工程还是主要依赖人的技巧而不是机器学习的结果，好的特征工程往往可以大幅度提高分类器的效果。（写到这里我想，那些在项目中能有95%以上准确率的，应该都是花时间做了特征工程的。到时候公布冠军的代码的时候，可以验证一下。）</p>

<p>除此之外，还有一些技巧。</p>

<ol>
<li>将特殊字符串替换成更知名的词条。举例来说，在化工类中，可能出现氯化钠、氧化钙等名词，假如没有类似的类的话。这些应是非常有区分度的特征，但是在分类过程中，首先分词器可能无法识别这种词，其次用chi-square也许能提取出这种特征，但是在tf-idf赋权的时候，他们的权重也不可能高（因为不会出现太多次）。假如把这些词统一替换成，“化学物质”类似的更知名的词条，那么化学物质就能是个很好的特征了。</li>
<li>有时候，词的部分和词的多词模式，也许是很好的特征。这很类似于我们在项目采用的2-gram、3-gram组合词。总的思路是先通过好的方法，去发现一些好的多词模式的候选，加入特征集之中，再利用常规方法去选择特征。具体用什么方法去挖掘好的多词模式以及如何使用。就目前来说，对我都是一个疑问。接下来，我也会有针对的性的读一些这方面的资料。</li>
<li>基于文章结构。比如特征出现在标题中，特征的权重翻倍。新闻类文章，特征出现在第一段或者每段的第一句话中，增加权重之类的。这就是基于位置的特征选择方法。在之前的Wikipedia的项目中，有一些类似的情况。</li>
</ol>


<p>综上可知，大致可做的工作就这么些。基本上对于每个文本分类项目，都有自己的特性，也许整体思路都是一样的。但是各个环节也都影响着最终的结果。结合我们的这个行业关键词分类的项目来看，我们可做的工作有：</p>

<ol>
<li>撰写人工规则。这个虽然可以提升效果，获得好成绩，但是目前阶段来讲，我更倾向于直接从别人那里获得做这个工作的最佳实践而不是自己去花大量的时间去实践。所以这个方向暂时不考虑，可以读一些方法性的文章。</li>
<li>领域特征选择。如何去做特征工程？如何可以系统的去选择领域特征。这是个需要研究的方法性问题。</li>
<li>如何选择和使用多词模式。凭借目前对这份数据的浅薄的理解，我们提出了一些方法，但是实践证明效果不好。我们首先，组合出所有可能的2-gram，然后利用2-gram出现的频率，以及2-gram的组成单元出现次数的比例，来筛选2-gram。选出来之后，我们就去根据这些2-gram去合并了原始的中文分词结果。这样相当于我们可能会淹没掉2-gram的每个组成单元的，可能的高区分度。这种方式显然不好，所以需要通过更多的阅读，来获得这方面的确定性的知识。而不是自己胡乱尝试。</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[行业关键词分类项目总结1]]></title>
    <link href="http://xutianming.github.io/blog/2013/11/14/text-classification-step-by-step/"/>
    <updated>2013-11-14T10:24:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/11/14/text-classification-step-by-step</id>
    <content type="html"><![CDATA[<p>在过去的一个月里，完成了一个行业关键词分类的项目，100w的训练数据，1000w的测试数据，33个分类。项目期限已经到了，但是我们的准确率目前只有88%，尝试了很多方法，遇到很多困难，也走过弯路。</p>

<p>项目结束并且效果不好，自然要寻找原因。一方面是等待比赛完全结束，公布冠军的解决方法，来开阔下眼界；另一方面就是更广泛的读一些相关的资料，做一些在项目过程中没有精力去做的阅读和学习工作。</p>

<p>首先简单描述下项目的过程。这次项目提供了33个分类，和一些要分类的关键词。每个关键词，提供了10个搜索引擎的自然排序的结果，以及购买这些关键词的用户。一部分已经标注的训练数据，和大量要分类的测试数据，分类效果的评估用的是宏平均(Macro-average)准确率。接下来，我们做了如下尝试：</p>

<ol>
<li>利用提供的文本数据进行文本分类。其中包括了利用lucene的IKAnalyzer进行分词、利用Chi-square进行特征选择、利用tf-idf作为权重生成文本的向量表示形式、利用Liblinear进行线性核的分类。第一次准确率87%。由于以上工作都是在Hadoop上完成的，加上学习和适应Hadoop平台的时间，其实只要一天的工作量的事情，做了2周（比赛承办方提供的Hadoop集群的限制实在是太多了）。</li>
<li>利用一些人工制定的规则，挖掘一些有着高区分度的词，利用这些词去修正上述文本分类的结果。这一步，把准确率提高到了88.5%。这也是我们最好的成绩了，本来可以利用这种方法获得更多的准确率的提升的，但是感觉可学习的东西很多，不想把更多的时间放在对数据的理解上。</li>
<li>利用关键词的用户购买信息去做关键词分类。这样做是建立在“购买关键词的用户相似，那么关键词倾向于属于同一类”这一假设的基础上的。所以想对&lt;关键词-用户>向量，应用分类算法。这一矩阵存在着维度过高（600w维）和过于稀疏的问题（很少有两个用户买同一个关键字）。所以推断直接分类既不可行，效果也不好。</li>
<li>这一条详细讲一下用户矩阵的使用。首先想到的是对用户聚类，以降低&lt;关键词-用户>向量的维度和稀疏性，使用k-means。但是由于上面所说的稀疏性问题，所以聚类效果也不好。于是想到利用第一次文本分类的结果，补充训练数据，另外首先对关键词聚类（已经聚好了，33类），这样&lt;用户-关键词>向量维度33维，并且不稀疏。可以使用k-means对用户进行聚类了，分别制定k为100和1000，进行聚类。在使用的过程中，发现了一个问题，k-means一般是基于距离的，而在这个项目的用户中，和质心距离相等的两个点，并不一定相同，距离计算过程中把所有的维度（关键词类别）等同看待，而现实中，我们购买了不同维度（类别）的关键词的用户，虽然距离相等，但是是属于不同类的。基于上述思考，我放弃了使用k-means，按照自己的想法，对用户进行了聚类。得到了2w类左右的用户。用户聚类之后，就达到了对&lt;关键词-用户>矩阵的降维和去稀疏的目的。可以应用分类算法了。</li>
<li>利用上述的方法，利用用户对关键词分类。但是结果非常不理想。原因在于，我们在第3步里的假设是不对的。也就是说，假如几个关键词，被几个相同或者相似的用户购买，并不代表关键词属于同一类。分析bad case可以看出，比如公司转让，服装公司转让被分入了服装类，房地产公司转让被分入了房地产类。但是这些关键词应该是被一个公司转让中介性质的用户购买了。所以从用户角度去分类，是不可行的。</li>
<li>经过上面的尝试，时间已经过去了3周。我们放弃了使用用户购买信息，试图提高文本分类的效果。首先我们发现，需要利用多词模式去提高特征质量，比如，搬家是个特征词，公司不是，但是搬家公司，是个非常有区分度的特征。由于没有事先的积累，也找不到现成的工具，我们自己根据经验和对数据的理解，写了bi-gram和tri-gram的map-reduce组合算法。使用频率对多词模式进行筛选，然后加入特征集训练。这一步，问题很多，因为大部分是拍脑袋定的，感觉很不科学，接下来，我可能会对这方面做一些针对性的阅读。</li>
<li>重复了文本分类的步骤，做了第二版的分类器，但是由于多词模式的筛选和使用都有些问题，所以这一版分类器没有取得预期的进步。</li>
<li>时间到了最后一周，感觉也没有什么特别好的点子了。偶然之中发现了Multi-class 分类相关的知识，比如one-vs-rest，one-vs-one等等。以为发现了新的方法，于是我们用Hadoop又做了一个one-vs-rest的多类分布式分类器，主要的思路是参考的别人的，花了很多时间去读别人的代码以及了解liblinear的各项接口和参数。这段时间算是对SVM和Hadoop编程有了更进一步的知识。无奈，最终发现，其实本来Liblinear做多类分类，就是用的one-vs-rest。而且我们的分类器，最终效果也不好。</li>
</ol>


<p>这就是过去一个月中，我们进行的所有工作。学到了很多知识，对SVM，NB分类器，对Hadoop的java接口和Streaming以及map-reduce的思想都有了进一步的认识，并且可以顺利使用了。虽然最终结果并不好，但是对于缺乏经验的我们来说，算是一个很好的经历。下篇文章里，我再结合一些书本上的知识，讲一些关于这个项目的思考，和可能的改进方向。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MapReduce初试]]></title>
    <link href="http://xutianming.github.io/blog/2013/10/29/mapreduce-program-trial/"/>
    <updated>2013-10-29T20:46:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/10/29/mapreduce-program-trial</id>
    <content type="html"><![CDATA[<p>最近在尝试做一个行业关键词分类的项目，训练数据百万级，测试数据千万。数据量不大，但是我的机器跑来已经相当吃力了，于是想用一些简单的Hadoop知识，体验一下这个目前业内最流行的分布式计算框架。</p>

<p>以前接触过很多分布式计算的知识，也学习过MapReduce。甚至实习期间，还多少使用到了hadoop的hive。但是一直都流于表面和概念，没有机会实践，甚至都不知道一个MapReduce程序如何来写，这次终于有机会尝试。</p>

<p>体验的过程并不顺利，为了简单起见，我使用Python和Streaming。参考网上的博客，很快就写出了MapReduce程序。这里的心得是，编写MapReduce程序并不比普通的编程难，关键是把算法分布式化，思考清楚，在Map和Reduce的过程中的数据流。这里分别附上我写的简单的Map，Reduce程序，作为以后工作的模板。</p>

<p>Mapper.py</p>

<pre><code>#!/usr/bin/env python
import math
import sys

features = []

fin = open("feature.txt","r")
for line in fin:
    winfo = line.split("\t")
    word = winfo[0].strip()
    features.append(word)
fin.close()


for line in sys.stdin :
    document = []
    info = line.strip().split("\t")
    for i in range(2,len(info)):
        document.append(info[i].strip())
    for feature in features:
        if feature in document :
            print '%s\t%s' % (feature, 1)
</code></pre>

<p>Reducer.py</p>

<pre><code>#!/usr/bin/env python
import math
import sys

current_word = None
current_count = 0
word = None

for line in sys.stdin:
    line = line.strip()
    word,count = line.split('\t')

try:
    count = int(count)
except ValueError:
    continue

if current_word == word:
    current_count += count
else :
    if current_word :
        print '%s\t%s' % (current_word,current_count)
    current_count = count
    current_word = word

if current_word == word:
    print '%s\t%s' % (current_word, current_count)
</code></pre>

<p>这一个统计DocumentFrequence的MapReduce程序。下面就一一描述下在编写这个程序中遇到的问题和解决方案。</p>

<ol>
<li>首先注意到Mapper中需要读入一个文件。一开始不懂文件应该怎么才可以被程序读到。尝试过放在HDFS和改变各种文件路径。其实只要在执行hadoop任务的时候，用<code>-file</code>参数指定就好了。</li>
<li>关于失败提示。<code># of failed Map Tasks exceeded allowed limit</code>,一开始的时候总是遇到。出现这个job失败提示的原因在于mapper程序有问题。执行失败，超过限度，job就会被杀死。所以mapper要做好本地测试。使用<code>cat input|python mapper.py|sort|reducer.py</code>可以对程序进行基本的测试。再就是在本地的hadoop平台上先跑一下，假如说没问题了，可以跑个5%，一般就可以提交到线上的集群了。</li>
<li>关于Streaming命令的各个参数。除了使用<code>-mapper</code>，<code>-reducer</code>之外。还可以利用<code>-D</code>选项，指定一些hadoop的运行参数，比如mapper和reducer的数量。这里有个比较重要的技巧。就是假如我们的执行逻辑中，只需要mapper不需要reducer。有几种办法，一个是不指定reducer，默认使用<code>IdentityReducer</code>，mapper的输入直接转化为reducer的输出；二是设置reducer数量为0，<code>-setReduceTasks=0</code>，有几个mapper即有几个输出文件。</li>
<li>最重要的一点，程序效率。上面给出的两个模板很高效。9GB的输入文件，十分钟即可在线上集群中完成。但是一开始并不是这样的。这个问题困扰了我整个周末。一开始，我想当然以为既然hadoop了，程序效率就不是短板了，我效率低，mapper分块小一点也很快完成，于是编程序的时候就很不在意效率。用list查找什么的，各种遍历循环，怎么好写怎么来。于是一开始我的程序测试好提交之后，job执行很慢，可以理解。令人崩溃的是，总是莫名其妙失败，我一度认为是线上集群不稳定弱爆了。后来在乐乐的提示下改进了程序的效率，list改用了hash，提高查找效率，去掉了几个循环。于是job很顺利也很迅速执行完了。事后分析，我觉得线上的集群可能做了很多的限制，我之前的写法，由于消耗太多的资源，由于本身优先级不高，就总是被delay，就会有莫名其妙的失败。所以mapreduce程序中，效率也是非常重要的，每一个task都高效执行，那么整个job都能很快完成，大大提高效率，节省时间啊！终于深刻认识到程序效率的重要性了。</li>
<li>最后一点，我还没有很好的解决。就是mapreduce程序的调试。hadoop实战这本书上有介绍debug的方法，我没怎么看懂，另外简单的程序，使用print大法打印一些调试信息和程序运行信息，很有利于快速定位bug。目前还不知道如何打印。如果在本地的hadoop的话，可以输出到<code>stderr</code>上，会最终输出到logs目录下的日志里。但是线上集群的话，就没有结点的访问权限，自然没法查看日志。所以我就想能否通过<code>Reporter</code>来打印一些消息。还没有尝试过，需要实践来验证。</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop环境搭建小结]]></title>
    <link href="http://xutianming.github.io/blog/2013/09/28/hadoop-first-trial/"/>
    <updated>2013-09-28T15:37:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/09/28/hadoop-first-trial</id>
    <content type="html"><![CDATA[<p>这两天一直在忙着搭Wikipedia Miner的环境，其中用到了Java和Hadoop相关。Java因为以前做过TopCoder，对于环境配置、Ant使用什么的还比较熟悉，也没有遇到什么大的问题。主要就是Hadoop了，虽然以前实习的时候，也多少接触过Hadoop，用过Hive写过一些Hql什么的，但是只是一些皮毛吧。自己搭建Hadoop环境还是头一回，这次要在我的Mac上，搭一个单机器的双节点伪分布式环境。具体的步骤，参考了官方的手册。</p>

<p><a href="http://sourceforge.net/apps/mediawiki/wikipedia-miner/index.php?title=Extraction">Wikipedia Miner搭建手册</a></p>

<p><a href="http://hadoop.apache.org/docs/stable/single_node_setup.html#PseudoDistributed">Apache Hadoop环境搭建手册</a></p>

<p><a href="http://wiki.apache.org/hadoop/Running_Hadoop_On_OS_X_10.5_64-bit_%28Single-Node_Cluster%29">OSX Hadoop环境搭建手册</a></p>

<p>步骤基本上是结合这几个看的，第一次搭建，就没考虑太多安全性，合理性。先搭个能用的环境体验下再说。具体步骤列举如下：</p>

<ol>
<li>安装JDK并配置JAVA_HOME(PATH,CLASSPATH在MAC上好像是不需要配置的)，<a href="http://blog.csdn.net/johnstrive/article/details/7791451">参考</a></li>
<li>开启mac的远程登录功能，也就是教程里的ssh。<a href="http://stackoverflow.com/questions/12369172/how-to-install-sshd-on-mac">参考</a></li>
<li>通过生成dsa秘钥等工作，ssh localhost可以不输入密码。<a href="http://wiki.apache.org/hadoop/Running_Hadoop_On_OS_X_10.5_64-bit_%28Single-Node_Cluster%29">参考</a>中讲ssh那段有。</li>
<li>下载安装Hadoop，然后配置。<a href="http://hadoop.apache.org/docs/stable/single_node_setup.html#PseudoDistributed">参考</a></li>
<li>运行Hadoop。</li>
</ol>


<p>上面列举出了我配置过来所参考的资料，我当然没有一次成功，每一步都有点小波折的，上面的资料都是我使用的，经过验证的方法。</p>

<p>接着我启动Hadoop，上传我的文件。总是抛异常，就像这个<a href="http://stackoverflow.com/questions/10097246/no-data-nodes-are-started">问题</a>里描述的，SO上有很多类似问题，解决方法也都一样，删除dfs里面的，name和data文件，然后重新format。但是我照做了就没成功，我也没有去读那些log，但是运气比较好的，我发现其实我的Hadoop根本就没有启动任何datanode成功。于是根据提示消息，我找到了这个<a href="http://stackoverflow.com/questions/6307244/hadoop-namenode-format-returns-a-java-net-unknownhostexception">问题</a>。最终我通过在hosts文件中，加了别名，解决了这个问题。</p>

<p>看起来很简单的过程，折腾了得有好几个钟头。到现在为止，hadoop基本上没问题了。但是总体的Wikipedia Miner的环境，还没搭建成功，还得解决各种Exception。等可以顺利使用Wikipedia Miner之后，我还要好好研究下，Wikipedia Miner的hadoop job部分是怎么写的。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[知识可视化项目第一阶段总结2]]></title>
    <link href="http://xutianming.github.io/blog/2013/09/24/summary-of-wikivis-project-2/"/>
    <updated>2013-09-24T21:15:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/09/24/summary-of-wikivis-project-2</id>
    <content type="html"><![CDATA[<p>今天和乐乐讨论了发表了Wikipedia Miner思路的论文，感觉颇深，故在此记录一下。</p>

<p>今天其实并没有讨论太多论文的内容，主要是基于项目第一阶段的反思，所以感觉还是很有意义的。这样的话，可以一步一步的提高我们解决问题的能力。关于面对一个要解决的课题的时候，我们从什么角度去思考它的解决方案，以及在目前的解决方案遇到了问题，走入了死胡同，如何调整路线，以最终找到那条正确的道路。</p>

<p>第一个问题是关于思考问题的角度。很明显，在第一阶段的项目进展中，我们并不成功，没有找到一种泛化能力非常好的算法，来对维基百科的数据进行有效的挖掘。在今天读了论文之后，发现论文作者和我们对于问题的思考思路是完全不同的。具体来讲，论文的课题是要解决“给哪些词加链接，有助于读者理解当前的文章”，而我们的目的是“有哪些关键字，可以很好的代表这篇文章，我们要给出这些关键字对应的维基百科链接”（这里仅指我们项目中的关键词打分步骤）。从这个角度看，我们要解决的问题其实是一致的，但是我们分别采用了什么样的思路来思考问题的呢？首先讲论文中的思路，维基百科中现有的有链接的文章是标注好的语料，那么满足什么条件的词是重要的关键字，需要加链接呢？我可以利用机器学习的方法来训练一个模型。首先根据经验选择一些特征，再利用有标注的训练数据，选择合适的算法，这样就可以学习模型了。这个模型告诉我们，在一篇文章里，那些词是关键字。那么，我之前使用的思路是什么呢？在语料充足的情况下，我们有很多统计量，比如卡方、平均值、标准差、余弦距离等等。那么用哪个统计量去描述关键词的特征最好呢？于是自然而然的想到了关键词提取，根据前人论文中提出的方法实现了。</p>

<p>这是两个解决问题的思路。不能说两个思路哪个好哪个坏，因为没什么可比性。这两种方法，前者立足于学习筛选关键字的标准，而后者立足于，我用什么样的标准，可以筛选出关键字。但是实践证明，解决我这个项目的问题，前者更好（Wikipedia Miner关键词挖掘效果很好）。因为在我的方法中，有个致命的弱点，就是语料。我是从一篇文章中根据统计规律去提取关键字，每个主题，使用什么词、什么词关键都满足一定的统计规律。那么语料不足怎么办？争取在不改变统计规律的前提下扩充语料，这一点太难做到了。而论文中的方法呢，学习的是标注关键字的方法，假定不论什么主题，哪些关键字重要，都是满足一定的准则的。显然后者更加符合维基百科的特点，详见<a href="http://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking">维基百科规则</a></p>

<p>下面讨论第二个问题，其实是个知识面的问题。因为最终乐乐发现，在我们读的这篇论文描述前人工作的时候提到了关键字提取相关的技术，在一篇综述性的文献中。而论文的作者，正是在前人工作的基础上，提出了这种方法，在2008年。按理说过去这么久了，为什么我们在最初确定技术方案的时候，没有找到现在这个更好的解决方案呢？因为我们使用的谷歌学术中没有收录那篇文章，并且在最初我们对于这个领域比较陌生的时候，很难使用准确的关键字去在搜索引擎上搜索，所以也就比较难一下子就找到最好的解决方案。也是经过了一段时间的研究和摸索，机缘巧合下，我的导师给我推荐了Wikipedia Miner。我才发现我们的问题已经有了这么优秀的解决方案了。</p>

<p>仔细想想呢，这其实涉及到了领域知识的问题。用什么技术，解决什么问题，是要经过长期实践积累经验的。随着阅历的提升，知道的办法越多，就越有解决问题的能力。在这一点上，我的导师自然大大的超越了我们。不过如何更有效的利用各种学术资源，会议门户啊、学术搜索啊、学术数据库等等，回头我还是要好好向我的导师请教一下。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[知识可视化项目第一阶段小结]]></title>
    <link href="http://xutianming.github.io/blog/2013/09/23/summary-of-knowledge-visualization-project-stage-1/"/>
    <updated>2013-09-23T14:44:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/09/23/summary-of-knowledge-visualization-project-stage-1</id>
    <content type="html"><![CDATA[<p>距离开始做数据可视化项目，已经过去了将近一个月的时间，目前项目的进展暂时告一段落，技术探索的工作基本完成，是个时候小结一下了。</p>

<p>目前项目的思路是这样的，以维基百科为数据源，将维基百科上的知识（目前以机器学习为例）用可视化的方式展现，提高学习和探索的效率。</p>

<p>具体来说，第一步。编写网页爬虫，从维基百科上下载数据，并且存储成结构化的数据（目前采用XML），存储的内容包括分类、关键词定义、页面关键词、参考文献。其中，关键词的定义，我们认为是维基百科词条页面的第一段，实际效果还不错，第一段文字往往能很好的直接阐释关键字的含义。页面关键词，就是那些本页面中比较重要的词，要想很好的了解本页面的内容，这些关键词对应的内容是非常重要的参考资料，在实现中，我们以页面内容中超链接的锚文字为基础，筛选那些链接指向的页面也包含本页关键字的链接锚文字，这一点是受到PageRank的思想所启发的；就效果来看，关键字筛选效果良好，但是这个条件明显太强了，在两个条件的限制下，大部分页面都只有7个以下的关键字，这样使得可视化的意义不大了，因为内容本来不多的情况下，单纯的文本就很容易理解的。参考文献，就是单纯是指维基百科词条页面中的参考文献，在目前的实现中，这些数据并没有加以利用。至于这些内容是如何从Web页面中提取的，主要靠对页面结构的分析，编写在网页爬虫的逻辑中。</p>

<p>第二步，是文本挖掘相关。针对上面的页面关键词，我们想要给每个页面关键词，根据相关度进行打分，然后根据这个相关度打分，进行进一步的可视化，比如比较重要的关键词Size比较大什么的。具体研究，使用了NLP中称为关键字提取（Keyword Extraction）的技术，通过阅读相关论文，确定了卡方相关性检验和仿PageRank的TextRank算法这两种技术方案。二选其一的话，我们选择了卡方检验，因为TextRank对我们来说有一些致命的缺点，比如计算量大、基于词在滑动窗口中的同时出现等等。相对来说，卡方检验对我们来说，要现实的多。</p>

<p>在我们参考的论文中，作者针对单篇论文，对关键字进行提取，打分自然就是卡方值。对语料数量的要求都不是特别高，但是对我们来说，我们现有的语料还是太少了，维基百科一般的词条页面中内容很少，用这些来进行关键词提取的话，估计效果甚微。于是我们决定扩充语料，经过研究，我们决定利用机器学习方面的会议(NIPS)及其提供的搜索引擎。因为项目做到这里我们已经决定先就一个词条，进行挖掘，实验下看下效果，所以就选了Machine_learning这个词条。扩充语料的过程还是用爬虫来完成的，模拟执行搜索引擎搜索，爬10篇论文下来作为语料。期间需要把pdf格式的转化成文本。经过一些折磨好歹完成语料的准备之后，开始进行关键词提取了，具体过程，一开始，我们想就第一步率先选出的关键字进行打分，算卡方值。但是语料准备好之后发现，我们选出的那些关键字，在这10篇论文中出现很少。究其原因，主要是我们扩充语料的思路有问题，试想维基百科中的词条，应该是比较宽泛的介绍性文字，而论文多是针对领域中某个小的问题的。从维基百科中挖掘出的关键词，很难应用到论文的语料中来。所以只能再次委曲求全，放弃对维基百科的关键字进行打分，改为直接从论文语料中进行关键词提取。</p>

<p>接下来的过程就是按照论文中的方法一步一步来了，涉及到词干提取，词组提取，词组聚类，计算卡方值等等。也顺利提取出了关键字，效果还不错。这么一来数据挖掘和准备的工作基本上就完成了。</p>

<p>第三步，可视化。可视化框架选择了D3js。用树状来表现分类，节点作为词条，点击词条，可视化展示此词条对应的关键词，使用了BubbleChart，主要涉及了D3js相关的javascript开发。</p>

<p>截止今天，以上的工作就是第一阶段做的事情。总的来说，没有成功的基于维基百科的数据进行挖掘，但是也探索了一条可行的路线，只是这条路线看样子扩展性并不好。于是，近几天忙于寻找一些更好的方式，不论是从数据源还是从挖掘的效果上。最近发现了一个专业的维基百科挖掘工具，(WikipediaMiner)[<a href="http://wikipedia-miner.cms.waikato.ac.nz/index.html">http://wikipedia-miner.cms.waikato.ac.nz/index.html</a>]和weka出自同一所大学，<a href="http://sepans.com/wikistalker/">效果</a>非常好。所以下一步，决定针对这个进行一些研究。先研究其原理，对比下我们挖掘的算法，看看差距在哪里，然后试图利用这个现有的工具，做一些工作。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python Style Guide]]></title>
    <link href="http://xutianming.github.io/blog/2013/09/22/python-style-guide/"/>
    <updated>2013-09-22T14:01:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/09/22/python-style-guide</id>
    <content type="html"><![CDATA[<p>写Python一段时间了，由于从初学到现在，一直还在熟悉语法和各种数据结构，以事先功能为主，自觉代码写的非常丑陋，现在也写了不少Python代码了，是时候挑战下习惯，遵循合适的代码风格，写简单易读的代码，Be Pythonic!!</p>

<p>本文主要参考了《<a href="http://google-styleguide.googlecode.com/svn/trunk/pyguide.html">Google Python Style Guide</a>》、《<a href="http://programmers.stackexchange.com/questions/119913/how-can-i-learn-to-effectively-write-pythonic-code">How can I learn to effectively write Pythonic code</a>》、《<a href="http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html">Code Like a Pythonista: Idiomatic Python</a>》三篇文章，摘取了其中我现在可能需要遵循的，因为有些做比较大的系统的项目才用到的，暂时就没写。我主要以写小算法和小工具为主。</p>

<p>一、编码风格：</p>

<p>1、不要使用分号，也不要利用分号把两个命令放在一行</p>

<p>2、每行代码不超过80（特殊情况下，尽量不要把链接分开两行放）</p>

<p>3、少用括号，条件语句和return语句不要用括号（特殊情况下，利用括号来进行断行可以）</p>

<p>4、缩进用四空格（一般在文本编辑器里设置tab转四空格）</p>

<p>5、空格使用。语法符号之后用空格，但是括号、花括号、方括号之后不要用。逗号冒号之后有个空格，但是之前不要有。赋值号前后有空格，但是在使用默认参数的时候，不要加。</p>

<p>6、一系列的赋值或者相同的操作。没必要用空格来使赋值号对齐。</p>

<p>7、字符串的使用。尽量使用string format和&#8217;%&lsquo;而不是&rsquo;+&lsquo;。在循环中构造字符串的时候，不要使用&rsquo;+&lsquo;和&rsquo;+=&lsquo;，而是把所有的字符串先存在一个list里，然后在循环结束之后，用join函数。（这一点需要格外注意，具体原因原文有）</p>

<p>8、多行string尽量不要用三个连续的引号，因为会破坏整体的缩进。使用join函数拼接。</p>

<p>9、file和socket使用完毕要显式关闭</p>

<p>10、临时的实现临时插入的代码，使用TODO注释</p>

<p>11、import的组织。每行一个，不要一次import多个。按照标准库、第三方库、程序特有的，这一顺序进行引用，统一级别按照字母顺序排序。</p>

<p>12、命名规范。module_name, package_name, ClassName, method_name, ExceptionName, function_name, GLOBAL_CONSTANT_NAME, global_var_name, instance_var_name, function_parameter_name, local_var_name。大部分使用了小写和下划线。</p>

<p>13、所有的脚本都应该是可import的，并且脚本被import之后，不能对当前脚本的功能有所影响。所以，脚本都应该有主函数！</p>

<pre><code>def main():
    …
if __name__ == '__main__':
    main()
</code></pre>

<p>不然的话，主程序的逻辑，会在被import的时候被执行。</p>

<p>14、注释。使用正确的注释方法。见原文。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[打表与预处理]]></title>
    <link href="http://xutianming.github.io/blog/2013/09/22/preprocessing/"/>
    <updated>2013-09-22T13:31:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/09/22/preprocessing</id>
    <content type="html"><![CDATA[<p>在ACM编程中，俗称打表，学名预处理。是我最近学习到的一个编程技巧。思想很简单自然，就是当有多组测试数据，但是每组测试数据的计算过程都是一模一样的。这时候，为了提高效率，节省计算资源，就会使用打表。</p>

<p>举例说明，计算斐波那契数列，可能要求输出第30-100项。算法中，我们会自底向上计算，也就是从第1项一直计算到第n项。每计算一项，都是重复这个过程。假如不对算法进行优化的话，我们计算第30项，要从第1项计算过来，计算第100项的时候，还要再从第一项计算一遍，这种情况下，就浪费计算资源了！</p>

<p>对于这种问题的优化思路，就是事先计算好所有可能需要输出的项，比如题目限制n&lt;=10000，那就把前10000项的斐波那契数列都计算好，存在数组里，然后根据测试用例的要求，输出对应的项就好了。</p>

<p>这种计算一次使用多次的思想，与动态规划非常相似。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[卡方校验、信息增益和互信息]]></title>
    <link href="http://xutianming.github.io/blog/2013/09/14/chi-square-information-gain-and-mutual-information/"/>
    <updated>2013-09-14T20:51:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/09/14/chi-square-information-gain-and-mutual-information</id>
    <content type="html"><![CDATA[<p>从一开始做文本分类到现在做的项目，一直在不断的学习新的知识，练习已经掌握的工具，也越来越深刻的认识到，“你所掌握的机器学习方法，并不是死板的一成不变的算法，而是一个一个的小工具，是解决问题的思路”。</p>

<p>这篇博客将对比三个在我进行文本挖掘相关工作的时候经常用到的小工具，以及我对他们的认识，仅限个人的粗浅观点，也是我当前的认识，不见得正确。</p>

<p><a href="http://zh.wikipedia.org/wiki/%E7%9A%AE%E7%88%BE%E6%A3%AE%E5%8D%A1%E6%96%B9%E6%AA%A2%E5%AE%9A">卡方校验</a></p>

<p>首先理解卡方校验的概念，除了维基百科，我总会看<a href="http://www.blogjava.net/zhenandaci/archive/2008/08/31/225966.html">这篇博客</a>。简单来讲，卡方校验是用来判别两个事件是否相关的方法，因此在特征提取、关键词提取中都有很多的应用。基本思路是，首先假设A、B不相关，然后根据A和B的联合概率分布，计算卡方值，公式在定义中都有给出，卡方值越大，越倾向于推翻原假设，也就是A、B越相关。在计算卡方值的过程中，最关键的问题是找到联合概率分布。那么我们应该如何理解卡方值呢？卡方值的含义其实是这样的，我们首先假设A、B不相关，也就是AB相互独立，那么我们可以为联合概率分布中的AB同时成立的事件计算期望值，根据定义公式显示，卡方值是这个期望值和实际值之间的差的平方除以期望值的和（定义和方差很像！），衡量了这两个量拟合的程度。那么，卡方值越大，代表期望值和实际值之间的差越大，也就是我们计算期望值所依赖的假设，是不成立的！</p>

<p>当我们想用卡方检验的方法，去判断两个事件是否相关的时候，理解问题的过程，就是建立事件A、B联合概率分布的过程，找到这个概率分布，就可以计算卡方值。举例说明的话，在文本分类的特征提取这一步，事件A是特征词w是否存在在本篇文档中，事件B是本篇文档是否属于类c。首先假设“w不是类c的特征词”，那么词w在属于类c的所有文档中出现的概率，应当与w在全部文档中出现的概率相同，我们依据这一假设，可以算出w出现在c类的文档数的期望，用期望值和实际值的差的平方除以作为卡方值。</p>

<p><a href="http://zh.wikipedia.org/wiki/%E7%9B%B8%E5%AF%B9%E7%86%B5">信息增益</a></p>

<p>也经常作为特征提取的方法，也用来衡量两个事件的相关性，但是是从信息论的角度来的。信息论用熵来衡量信息量。信息增益，简单理解就是在已知条件B的情况下，事件A的信息量的改变程度，就是熵的改变程度。同样有<a href="http://www.blogjava.net/zhenandaci/archive/2009/03/24/261701.html">一篇博客</a>讲得很透彻。从上面的描述可以看出，计算信息增益的关键是计算熵和条件熵。所以说，信息增益对应的概率分布，是普通的概率分布和一个条件概率分布。那么我们应该如何具体理解信息增益呢？我们首先计算事件A的信息熵，然后考虑事件B，计算在有事件B的情况下的条件熵，两者之差，就是信息熵的改变，改变越大，说明事件B对于事件A的相关性越大。由于涉及到信息论，所以信息量改变这一点，就变的略显抽象。</p>

<p>仿照上文中对卡方校验的介绍。利用信息增益解决问题的关键，在于找到这个条件概率分布，以计算条件熵。同样举文本分类中特征提取这个例子。事件A是指文档属于类ci，事件B是词w是否出现在文档中。根据P(Ci|w)和P(Ci|^w)这两个条件概率分布，可以计算条件熵H(C|T)。</p>

<p>不过仔细读过信息增益的概念，发现我们上面介绍的和KL散度差别挺大的，目前我也不能理解这两种概念是否本质上相同，但是在分类和挖掘领域，大家一般是像我说的那样去理解信息增益的。</p>

<p><a href="http://zh.wikipedia.org/wiki/%E4%BA%92%E4%BF%A1%E6%81%AF">互信息</a></p>

<p>在互信息定义的多条公式中，我一般使用第三个，就是X，Y熵之和减去联合熵。根据联合熵，就可以发现，决定互信息的，也是一个联合概率分布。</p>

<p>比较形象的理解互信息的公式方法是，认为事件X和事件Y相关，可以让总信息熵减少多少，差别越大，X和Y越相关。计算互信息的关键，是计算联合熵，因此在用互信息解决问题的适合，找到这个联合概率分布，是核心。不要试图去寻找其他的概率分布，实践表明，互信息的联合概率分布与卡方检验的非常相似，都是x是否怎样，y是否怎样。同样以文本分类为例，X为文档是否属于类c，Y事件为文档是否包含w，然后利用这个概率分布去计算联合熵。这么一看，和卡方检验几乎完全相同！只是利用了不同的理论基础。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[动态规划]]></title>
    <link href="http://xutianming.github.io/blog/2013/09/02/dynamic-programming/"/>
    <updated>2013-09-02T19:53:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/09/02/dynamic-programming</id>
    <content type="html"><![CDATA[<p>当遇到复杂问题的时候，我们很自然的想要降低问题的复杂度和规模。在《算法导论》中，作者将动态规划和分治算法都当做降低问题规模，将复杂问题转化为子问题的方法。不同的是，分治算法划分出的子问题都是独立的，而动态规划中的子问题相互之间是非独立的，高层的子问题（离原问题比较近）往往依赖于低层子问题。所以假如不利用动态规划，自底向上计算这类问题的时候，会有大量重复的计算，从而使算法的效率低。</p>

<p>那么，动态规划的本质就是建立一个表，存储子问题的结果，在计算高层问题的时候，直接利用表中的结果。这就是动态规划的基本思想。动态规划往往被用来计算最优化问题，利用的理论是子问题最优化理论，也就是一个最优化解的路径上的所有子问题，也都是当前最优化的。但是，这已经不是动态规划的核心了，我之前就在理解动态规划的时候，犯了这个错误。在试图利用动态规划去解决问题的时候，我往往纠结于去寻找这样的最优化子结构。其实有时候，这种最优化子结构并不好找甚至没有，但是也可以利用动态规划去解。动态规划的基本思想，还是很朴素的。</p>

<p>最近在TopCoder上读了一个黄名大牛写的<a href="http://www.topcoder.com/tc?d1=tutorials&amp;d2=dynProg&amp;module=Static">动态规划进阶手册</a>深受启发。文中指出，动态规划应该这么描述：动态规划算法往往建立在一个递归方程和这个方程的初始状态之上（很像那种找递推公式的问题）。每个子问题，都建立在之前发现并计算好的问题上。动态规划问题往往都有多项式级别的复杂度，因此比暴力搜索和回溯算法效率要高（这一点我还不是特别明白，文中有提出，多项式复杂度的问题都可以用动态规划来解决）。</p>

<p>动态规划问题的解决过程是分为两步：定义问题的状态（或者说是子问题），发现如何又一个状态转移到下一个状态（也就是定义状态转移方程，是上文指出的“递归方程”），更准确的说，如何利用已有的子状态，计算下一个状态，因为有时候不是从i转移到i+1。</p>

<p>举例说明：</p>

<p>给定N个硬币，价值分别为（v1，v2，v3&hellip;vN），给定价值和S。求解价值和为S的硬币集合，使其包含的硬币个数最少，每种价值的硬币可以使用任意多个。</p>

<p>问题分析：</p>

<p>在这个问题中我们要求解的结果是价值和S，那么定义“状态”为当前价值和为i（i&lt;S），对于所有的状态j（j>i），要利用i来求解，而对于状态j(j&lt;i)，要在计算状态i之前计算好。在实践中，我发现子状态往往是针对要求解的那个问题的。</p>

<p>对于状态i，我们假设，所有的子状态j（j&lt;i）都已经计算出结果来了。那么i如何利用子结果来计算。对于所有价值小于i 的硬币（循环遍历）j，使用此硬币，使价值和变为i-j，由于i-j&lt;i，那么此子问题，我们应该已经计算好了，假设硬币数为m，则i状态的结果应为m+1。遍历过程中，我们把最小的那个结果存给状态i。</p>

<p>基于这种想法，我们需要从状态0开始，到状态s逐一计算。也就是动态规划中所谓的，从顶向下思考，从底向上计算。这里有个思考陷阱，千万不要想如何从i计算i+1，而是如何利用已有的所有子状态j(j&lt;=i)计算i+1。就写这篇博客的过程，我因为想当然认为这样，差点进入死胡同。</p>

<p>这是动态规划最基本的思考过程。</p>

<p>一般情况下，我们要遍历所有的比i低的状态j，但是并不是所有的j都能转换到i。在这个硬币问题中取决于，是否有个硬币k，使得S[j]+vk=S[i]。这就是状态转移的条件，所以在实践中，我们一般要判断状态j能否转移到条件i，在上例中没有判断的原因是，我们根据硬币价值遍历，遍历到的都肯定满足转移的条件。其他问题，比如无向图中，状态节点j能否转移到节点i取决于i和j之间是否有一条边。这就要判断了，当然假如存储图结构的时候，存储了所有指向节点i的入节点j，就可以遍历这个节点集了，但是对于自底向上的动态规划解决方案中，很少出现。</p>

<p>更复杂一点，状态转移不是一维的，变成二维甚至多维（目前只见过二维）。这种问题，通过比较好的定义子状态，也很好理解。</p>

<p>还有其他的情况下，限制条件所需的信息和我们所求的结果并不一致。比如赋权图中，从节点j到节点i的最短路径，在权值和小于s的前提下。我们最终关心的是路径长度，但是状态转移的限制条件是权值和，这时候需要一个二维数组来存储。</p>

<p>更复杂的情况，乍一看并不是动态规划问题，但是和我们以前遇到过的动态规划问题很像，这时候，我们需要一些转变，把问题转换成以前的问题的升级版去求解。这是最难的，菜鸟级别的我暂时还没什么感受。留在日后补充吧。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[为控件增加鼠标悬浮提示]]></title>
    <link href="http://xutianming.github.io/blog/2013/08/26/tooltips-on-mouse-hover-using-d3/"/>
    <updated>2013-08-26T14:21:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/08/26/tooltips-on-mouse-hover-using-d3</id>
    <content type="html"><![CDATA[<p>上星期阅读了《Interactive Data Visualization for the Web》一书之后，便开始尝试使用D3js了。以前一直没怎么写过前端和界面，个人对于坐标位置布局什么的相当没感觉，总是乱乱的。所以这次写来，大部分也都是用的别人写好的现成的东西，在这个基础上做些改动。</p>

<p>今天做的改动之一就是为标签Text增加鼠标悬浮事件，提示改概念的定义。</p>

<p>效果图如下：</p>

<p><img src="http://i.imgur.com/0E53nrC.jpg" alt="Imgur" /></p>

<p>这段代码参考了StackOverflow上的这个<a href="http://stackoverflow.com/questions/10805184/d3-show-data-on-mouseover-of-circle">帖子</a></p>

<pre><code>// Show defination of the current item
function show_defination(d) {
    d3.select(this)
        .append("svg:title")
        .text(function(d) {return d.def;})
        .attr("x",function(d) {return d.x+10;})
        .attr("y",function(d) {return d.y+10;})
}
</code></pre>

<p>我添加的代码如上，绑定在text控件的mouseover时间上。<code>on("mouseover",show_defination);</code></p>

<p>另外我还发现了一个很好的在线校验json的<a href="http://jsonlint.com/">网站</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ACM中递归问题解决方法思考]]></title>
    <link href="http://xutianming.github.io/blog/2013/08/20/recursion-problem-and-hmm-model/"/>
    <updated>2013-08-20T21:47:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/08/20/recursion-problem-and-hmm-model</id>
    <content type="html"><![CDATA[<p>最近在做ACM的练习题来练习编程的基本功，今天研究到了递归问题。就是通过寻找递推公式，来解决问题。</p>

<p>在总结方法的过程中，发现，对于一些比较简单的题目，可以在纸上比较轻易的模拟过程的问题，可以通过观察第n项与前面的1-3项之间的关系，直接得到递推公式。</p>

<p>比如：</p>

<p><a href="http://acm.hdu.edu.cn/webcontest/contest_showproblem.php?pid=1001&amp;ojid=0&amp;cid=1531&amp;hide=0">超级楼梯</a></p>

<p><a href="http://acm.hdu.edu.cn/webcontest/contest_showproblem.php?pid=1002&amp;ojid=0&amp;cid=1531&amp;hide=0">一只小蜜蜂&hellip;</a></p>

<p>这些问题都比较简单直观，所以很容易思考直接获得递推公式。</p>

<p>而其他的一些问题，可能就相对来说抽象一些，需要我们画图，找一下规律，这时候千万不要怕麻烦，画图可以很直观的帮助我们找到规律，也就是递推公式。</p>

<p><a href="http://acm.hdu.edu.cn/webcontest/contest_showproblem.php?pid=1004&amp;ojid=0&amp;cid=1531&amp;hide=0">骨牌铺方格</a></p>

<p><a href="http://acm.hdu.edu.cn/webcontest/contest_showproblem.php?pid=1009&amp;ojid=0&amp;cid=1531&amp;hide=0">Tiling_easy version</a></p>

<p>比如上面这种铺砖的，画图之后，有目的的去观察，有n-1和n-2项如何去铺砖得第n项，就可以发现递推公式。这种一般从第三项之后，才会有规律。也就是找第四项和第三、第二项之间的关系。</p>

<p>总而言之，这类问题考验思维能力要强于对编程能力的考验。思考的方法更为重要。我大体总结了一下，这类问题主要有两种思考的方式，一种是从前往后，一种是从后往前。</p>

<p>从前往后，就是考察第一、第二项，这一般是递推公式的初始项，然后考察第三项、第四项、第五项，在计算这几项的时候，看能否提出一种可以推广的计算方法，这种方法，抽象来，就是递推公式。这种方法的优点是，比较直观，可以画图，容易理解，缺点是，有些问题没法解决。</p>

<p>从后往前，就是直接考察第n项的得出，有几种可能来构造第n项，每种可能如何用第n-1项、第n-2项的值，来构造。这种方法的优点是通用性更高一些，只要脑筋转的过来，大部分问题都可以通过这种方法找出递推公式。缺点就是比较抽象，不怎么好想。</p>

<p>写到这里，我觉得这种问题的解决，好像是HMM中的前向算法和后向算法，分别就是从前向后和从后向前，进行状态的转移。不过再仔细思考，这也只是形似而已，似乎在本质上，双方没有数学层面上的必然联系。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[对数据可视化的初步认识]]></title>
    <link href="http://xutianming.github.io/blog/2013/08/19/my-general-view-on-data-visualization/"/>
    <updated>2013-08-19T14:06:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/08/19/my-general-view-on-data-visualization</id>
    <content type="html"><![CDATA[<p>先说点废话，导师是研究数据可视化方向的，所以平时在实验室的工作大部分是和数据可视化相关联的。每天至少也会花六七个小时在相关的方面。大四一年七七八八做了很多项目，很少有和数据可视化相关的，个人感觉也都没啥技术含量，觉得大四一年基本上白忙活了，自己也没提高多少。研一的时候开始接触了数据挖掘和机器学习，也去公司实习，学习了很多这方面的知识，对这个方向也有了大致的了解，非常喜欢，很有些相见恨晚的感觉。为什么呢？主要是惊诧于数据的强大力量。说到数据的价值，其实一堆一堆的数字、文字等等，是没什么价值的，价值在于数据中蕴含的信息。</p>

<p>机器学习和数据挖掘，就是这样的工具，可以把信息，从浩瀚的数据海洋中提取出来。海量数据的信息处理，就是最近很火的“大数据”了。在未来，互联网向各个传统行业，我们生活的方方面面不断渗透的过程中，数据挖掘和机器学习的方法，是可以做很多很多的事情的。</p>

<p>再说数据可视化，以前的时候理解一直很不到位，一直觉得数据可视化，就是用一种很好的方式，来展现信息，表达观点，这样可以提高沟通的效率，也很符合人的认知习惯。那么数据可视化，应该技术含量很低，更多的是个经验学科，自己领域里的数据，一般用什么样的形态来表示，这个从业时间长了，自己就了解了。这两天在看《Interative Data Visualization for the Web》这本书（中文好像叫《D3js实战》么？），只看了一点，就有很大收获。但从对数据可视化的由来和认识来讲，感觉作者理解的好深刻。数据可视化分为解释性的可视化（explainatory）和探索性的可视化（exploratory）。前者就是我一直认为的那样，而后者我导师一般说是“可视分析”，通过提供对数据的交互手段，帮助人们挖掘知识、信息。</p>

<p>那么数据可视化和数据挖掘机器学习应当是有着非常密切的关系的。先说解释性的可视化。一般来讲，我们说数据挖掘机器学习是非常强大的工具，可以帮助我们从数据中学习，以进行分类、回归、聚类等工作，解决实际问题。那么挖掘出来的信息，需要交流和传递，才能有产生价值，这时候，数据可视化可以帮助你，更好地把你想表达的信息传递出去。那么可视分析呢，意义在于，有些问题，数据挖掘可能帮不上忙，但是人们的长期工作以来，经验积累，有很多先验知识，这些知识一般比较难在数据挖掘的过程中有所帮助。那么可视分析就可以帮助这些有经验的人士，更好的操作数据，可以快速的产生数据的不同视图，让其他行业的专家也可以享受到挖掘的福利。这方面，美国已经有企业做的很成熟了，公司也都上市了，Tableau。回头我也要试用学习一下。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[C++输入输出总结]]></title>
    <link href="http://xutianming.github.io/blog/2013/08/07/c-plus-plus-input-and-output/"/>
    <updated>2013-08-07T09:23:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/08/07/c-plus-plus-input-and-output</id>
    <content type="html"><![CDATA[<p>输入：
1 . 未说明有多少个input block</p>

<pre><code>while(scanf("%d %d",&amp;a,&amp;b)!=EOF) 
{    ...    }
</code></pre>

<p>scanf返回的是输入的变量的个数，EOF=-1表示没有输入</p>

<pre><code>while(cin&gt;&gt;a&gt;&gt;b) {    ...    }2 . 说明有N个input block    

scanf("%d",&amp;n) ;    for( i=0 ; i&lt;n ; i++ )     {...} 
    cin &gt;&gt; n;   for( i=0 ; i&lt;n ; i++ )     { .... } 3 . 输入是一整行的字符串
C  
char buf[20];     gets(buf); 

// 如果用string buf;来保存：
getline( cin , buf );     // 如果用char buf[ 255 ]; 来保存：     cin.getline( buf, 255 );
</code></pre>

<p><code>scanf(“ %s%s”,str1,str2)</code>，在多个字符串之间用一个或多个空格分隔；若使用gets函数，应为gets(str1); gets(str2); 字符串之间用回车符作分隔。通常情况下，接受短字符用scanf函数，接受长字符用gets函数。而getchar函数每次只接受一个字符，经常c=getchar()这样来使用。getline 是一个函数，它可以接受用户的输入的字符，直到已达指定个数，或者用户输入了特定的字符。它的函数声明形式（函数原型）如下：    <code>istream&amp; getline(char line[], int size, char endchar = '\n');</code>不用管它的返回类型，来关心它的三个参数：
* char line[]： 就是一个字符数组，用户输入的内容将存入在该数组内。<em> int size : 最多接受几个字符？用户超过size的输入都将不被接受。</em> char endchar :当用户输入endchar指定的字符时，自动结束。默认是回车符。例如：char name[4];cin.getline(name,4,&lsquo;\n&rsquo;);由于 endchar 默认已经是 &lsquo;\n&#8217;，所以后面那行也可以写成：cin.getline(name,4);输出：
1 . 输出之后换行    C:    {     &hellip;.     printf(&ldquo;%d\n&rdquo;,ans);     }     C++:    {     &hellip;     cout &lt;&lt; ans &lt;&lt; endl;     }
2 . 每个输出之间有空行</p>

<pre><code>  while(scanf("%d %d",&amp;a, &amp;b) != EOF)           printf("%d\n\n",a+b);    
C++:    cout &lt;&lt; ans &lt;&lt; endl &lt;&lt; endl;
</code></pre>

<p>3 . 输出结束之后输出空行</p>

<pre><code>C:    for (k=0;k&lt;count;k++) {     while (…) {              printf(" %d\n",result);       
  }       
  if (k!=count-1) printf("\n");     
}     C++   // 类似，输出语句换一下即可。
</code></pre>

<p>在学习了以上基础之后，我在HDUOJ上做了十几个题目来练习，也犯了不少错误，总结如下，以避免以后再犯，提高编程的成功率和效率：
* 未说明有多少个输入块时，scanf要和EOF比较，这个和cin的使用不同，不然的话OJ上会超时
* 同样的道理，也不要用<code>while(true)</code>来代替<code>while(cin&gt;&gt;a)</code>，也会在输入结束的时候，仍然执行从而超时
* 再就是输出有换行，要输出两个<code>endl</code>的时候，要注意假如指定输入个数，最后一次输出之后，不要再换行的。没指定有多少个输入的时候，则每次输出都换行。
另外，我还犯了其他一些低级错误，比如数据类型定义错误等等，这些错误自己调试的时候，不会出错，但是OJ上指定的某些测试用例肯定是没法通过的。所以要格外小心，OJ也不会告诉你哪个测试用例挂了，就很浪费时间，为了提高效率，写程序的时候就得多注意下，看清题目。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[姥爷的老房子，我的童年]]></title>
    <link href="http://xutianming.github.io/blog/2013/07/24/bye-my-childhood/"/>
    <updated>2013-07-24T12:59:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/07/24/bye-my-childhood</id>
    <content type="html"><![CDATA[<p>前几天，姥爷去世了。最疼爱我的姥爷，离开了。这几天一直在忙着各种后事，今天终于得以清闲，和爸爸妈妈一起给姥爷打扫房间，处理一些剩余的食物，几年来，也是第一次再好好看看这栋房子，也许，以后再也没机会了吧。</p>

<p>现在居住的大院里，几乎有我童年的所有回忆，从幼儿园起，我就在这所大院里玩，幼儿园-学前班-小学，直到小学五年级，我们家也搬了进来，住在姥爷家对面的楼上，于是，我的初中-高中也在这里度过。直到我考上大学，18岁，离开了泰安，去往武汉，再来到北京。一晃已经5年过去了，我也已经23岁了。</p>

<p>我生命的前十八年，几乎都是在这座大院里度过，几乎都和姥爷家密不可分，那时候，我几乎每天每星期，都会过来。都会亲亲姥爷的脸，都会吃姥爷给我买的零食。但是，随着姥爷的离开，也许过不了多久，我就要和这栋老房子说再见了。</p>

<p>我想在我的博客里，留下这些记忆。</p>

<p><img src="http://i.imgur.com/sm0cACZ.jpg" alt="Imgur" /></p>

<p>那时候，每天放学，我都会飞奔到姥爷家，看会报纸，吃会零食，放自行车，或者取家里的钥匙然后回家。姥爷经常会在我放学的点，站在门口的石榴树下等我回来，帮我停自行车，好让我赶紧回家休息。</p>

<p><img src="http://i.imgur.com/bquu6Dd.jpg" alt="Imgur" /></p>

<p>姥爷家的碳池子，秋天囤积碳，以供冬天取暖，不过已经废弃太久了，我记事起，这个就没用过，但是这成了我的游乐场所，我经常爬上爬下，从下面的小门里钻进钻出。</p>

<p><img src="http://i.imgur.com/DSjVd2p.jpg" alt="Imgur" /></p>

<p>姥爷门前的石榴树，也是我的乐园。由于疏于打理，每年夏天石榴树都会结很多的果子，但是秋天不到，大部分果子就都掉了。到了果实成熟的时候，往往只有几颗果子，这时候姥爷会分给我们几个小孩一人一个，超级甜。</p>

<p><img src="http://i.imgur.com/6hCeTbQ.jpg" alt="Imgur" /></p>

<p>正是7月底，就有些石榴炸开花了。大部分石榴都留不到秋天。</p>

<p><img src="http://i.imgur.com/LcbfokW.jpg" alt="Imgur" /></p>

<p>石榴树是很多蚂蚁的家。有时候我是这些蚂蚁的朋友，但大部分时间，我是他们的灾难。我会用开水烫，用脚踩，用针扎；偶尔无聊的时候，我会在小院里用蝇拍打苍蝇，喂他们，而且一般都不把苍蝇完全打死，看着苍蝇被蚂蚁撕咬。</p>

<p><img src="http://i.imgur.com/W74Vt1p.jpg" alt="Imgur" /></p>

<p>这是姥爷请人搭建的小储藏室，是他的百宝箱。他会把各种暂时用不到的东西都放进去，里面堆满了东西，但是都整齐的码在四周的墙上，屋子的中间还是留了很大的空地。姥爷是个仔细的人，什么都一丝不苟，力图做到干净整洁。墙上，还留有张贴姥爷讣告的痕迹。</p>

<p><img src="http://i.imgur.com/zIsTacF.jpg" alt="Imgur" /></p>

<p>进门是个小走廊，墙上的油迹还在，姥姥在世的时候，经常会在这里生火做饭，用小小的蜂窝煤炉子，想来现在应该再没人烧蜂窝煤了吧。那已经是十几年前的事情了。</p>

<p><img src="http://i.imgur.com/KuSvvfp.jpg" alt="Imgur" /></p>

<p>我长个子的时候，姥爷仔细的在墙上记录着我身高的变化，那是我小学的时候，等高中了，我已经比姥爷还高了。亲姥爷的时候，都需要弯下腰，姥爷要拍我的后脑勺，也得拉着我的脖子才可以。</p>

<p><img src="http://i.imgur.com/VQA7ptN.jpg" alt="Imgur" /></p>

<p>姥爷家的小院子，我的乐园。屋子窗下，姥爷搭的小台子，摆放一些杂物和花盆，台子下面，则是我的藏宝库，我把各种脏兮兮的玩意，藏在这下面，第二天继续玩。我在这里冻过冰，养过蝌蚪。</p>

<p><img src="http://i.imgur.com/xuEi1WQ.jpg" alt="Imgur" /></p>

<p>小院里的水龙头，姥姥会在这里洗菜洗衣服，这样不会弄脏屋子。而这里，是我的弹药补给地，那时候喜欢玩水，水枪、喷壶等等，石榴树上的蚂蚁，自然成了我的攻击对象。</p>

<p><img src="http://i.imgur.com/XUVWbUS.jpg" alt="Imgur" /></p>

<p>小院里的排水沟，姥爷自己通的，我经常在这里撒尿，一直到年纪很大了还这样，在墙上画地图，每次姥爷都会笑着骂我，但是从来没制止过我。</p>

<p><img src="http://i.imgur.com/gJSWPF4.jpg" alt="Imgur" /></p>

<p>进了屋子，是姥爷平时坐着喝茶的地方，最早的时候，姥爷和姥姥会在这里说话，后来只剩下了姥爷，再后来，姥爷也离开了。妈妈说我刚生下来，可以横着躺在沙发上，头脚都不会够到扶手。</p>

<p><img src="http://i.imgur.com/RxQrOYO.jpg" alt="Imgur" /></p>

<p>沙发旁的壁橱，姥爷放各种点心饮料的地方，每次来，我都会钻进壁橱，找我喜欢吃的东西。</p>

<p><img src="http://i.imgur.com/yL04Xdk.jpg" alt="Imgur" /></p>

<p>黑色的老鹰，一直都在，展翅雄飞。姥爷一直爱用的纸日历，已经好久没人撕了。小时候，每天早上我就会着急的撕下当天的日历，可是现在才明白，被撕去的日历，就是过去的时间，再也回不来了。</p>

<p><img src="http://i.imgur.com/VVZ2qox.jpg" alt="Imgur" /></p>

<p>姥爷的卧室，一如既往的整洁。姥爷每天下午的时光都是在这里度过的，睡了午觉，然后坐在写字台前练字，计划最近要做的事情，读报纸杂志。小时候，我趴在床上看《新白娘子传奇》的样子还历历在目，只是那时候的电视好小，也只有几个电视台。</p>

<p><img src="http://i.imgur.com/sOLBZFz.jpg" alt="Imgur" /></p>

<p>写字台的玻璃下面，会压着姥爷要记的事情，一些生活常识以及保健知识。还有所有孩子的电话。里面没有我的电话，因为一般都是我给姥爷打电话，而且我在外上学，号码常换。</p>

<p>物是人非，姥爷离开我们了，再也没人给我压岁钱，亲切的问我钱够不够花，学习情况怎么样，叮嘱我好好学习了。很快，爸爸妈妈也会搬离这个大院，住到新房子里，这里的样子，会越来越模糊吧。可是，这几乎我对泰安这个小城所有记忆的缩影，我18年最主要的记忆，都是和这里相关的。人总会长大，我也已经离开泰安5年了，还不知道以后会生活在哪个城市，但是应该很难再回来吧。</p>

<p><img src="http://i.imgur.com/8xQVz35.jpg" alt="Imgur" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[核函数理解]]></title>
    <link href="http://xutianming.github.io/blog/2013/07/20/understanding-kernel-function/"/>
    <updated>2013-07-20T11:13:00+08:00</updated>
    <id>http://xutianming.github.io/blog/2013/07/20/understanding-kernel-function</id>
    <content type="html"><![CDATA[<p>SVM相关的理论知识看过几遍，但是感觉一直都不是理解的很透彻，特别是核函数的概念。今天又查阅了大家写的学习笔记等资料，结合自己的实践经验，感觉对核函数的理解更进一步了，所以记录在这里。</p>

<p>一句话说明核函数的意义和作用：为使非线性可分问题转化为线性可分，将特征向量映射到高维，映射函数所满足的性质。</p>

<p>具体说来，svm可以直接解决线性可分的分类问题，对于近似线性可分的问题，通过增加松弛变量，也可以实现分类。那么对于非线性可分的问题呢，比较直观的想法是映射，也就是通过把特征映射到高维，使非线性可分问题转化为线性可分的问题。但是就一般的实际问题来讲，寻找这个映射很困难，而且很难有可推广的方法。于是提出了kernel trick。可以绕过寻找这个映射函数。</p>

<p>在理论公式的推导中，可以算出参数w的表示方法：</p>

<p><img src="http://i.imgur.com/KqPVcLl.png?1" alt="Imgur" /></p>

<p>于是分类超平面的函数也可以写成：</p>

<p><img src="http://i.imgur.com/UVGLjMS.png?3" alt="Imgur" /></p>

<p>这个公式中只有&lt;x(i),x>的内积形式，也就是说，我们映射之后，需要计算&lt;Fi(xi),Fi(x)>的内积。而我们定义的核函数，就是这一内积的形式。</p>

<p><img src="http://i.imgur.com/lLicBOp.png?1" alt="Imgur" /></p>

<p>如此一来，我们就可以用核函数直接替换内积，相当于，我们把原始的特征向量，映射到了高维，并且避免了寻找映射函数。而libsvm中推荐使用的RBF核函数，就是一个有着良好特性的核函数。可以将样本映射到一个更高维的空间，可以处理非线性的分类问题。而其关键的参数c和g分别为惩罚因子和核函数的参数。（RBF 核K(x, y) = exp(－γ || x －y ||的平方),γ > 0，g为gamma，就是r）。我们在使用libsvm训练之前，应该首先寻找最优的参数c和γ。</p>
]]></content>
  </entry>
  
</feed>
