---
layout: post
title: "模型学习小结"
date: 2014-03-01 22:16
comments: true
categories: 机器学习
---

近来我对机器学习的模型有了一些更接近本质的认识。也让机器学习的这些理论，在我的眼里，变得不那么神秘。

机器学习的理论，可以分为两个部分，一部分是建模，一部分是求解模型。所有的机器学习算法，逻辑斯蒂回归、支持向量机，本质上来讲，都是一套建模的方法和一套可行有效的解法。

建模，从数学的角度，是对实际问题的描述。比如逻辑斯蒂回归利用逻辑斯蒂分布的特殊性质建模，也就是odds ratio为线性函数。而odds ratio是本sample为正例和负例的概率的比值的log。这样odds ratio的正负，可以代表本sample为正例的概率大还是为负例的概率大。而odds ratio的正负，就是线性函数的正负。因此有了逻辑斯蒂回归。建模的任务是描述问题，并得出目标函数，也即是Loss Function。Loss Function经常利用极大似然估计（Max likelyhood）和最小平方误差（Least squares）写出。支持向量机也是如此，支持向量机的建模过程中，认为sample到分类超平面的距离，衡量分类的置信程度。那么定义符合要求的分类超平面为，数据集中所有的点，到本分类超平面的距离，都大于等于R。然后极大化R，得到目标函数。

得到目标函数之后，就要求目标函数的极值。数学里的最优化问题，有非常多的方法。常见的有梯度下降法、牛顿法、拟牛顿法。理论上来讲，求解和建模是独立和分开的，也就是说，上面建模得到的目标函数，可以用任意一种最优化解法来解。当然这只是最理想的情况。实践中，每一种模型（目标函数），几乎都对应了一种实际可行的，可计算的解法。

从这种比较高层次的角度，再去读那些机器学习的书籍，会更加从容，应该可以把握的更好。