---
layout: post
title: "知识可视化项目第一阶段小结"
date: 2013-09-23 14:44
comments: true
categories: [数据可视化,文本挖掘]
---

距离开始做数据可视化项目，已经过去了将近一个月的时间，目前项目的进展暂时告一段落，技术探索的工作基本完成，是个时候小结一下了。

目前项目的思路是这样的，以维基百科为数据源，将维基百科上的知识（目前以机器学习为例）用可视化的方式展现，提高学习和探索的效率。

具体来说，第一步。编写网页爬虫，从维基百科上下载数据，并且存储成结构化的数据（目前采用XML），存储的内容包括分类、关键词定义、页面关键词、参考文献。其中，关键词的定义，我们认为是维基百科词条页面的第一段，实际效果还不错，第一段文字往往能很好的直接阐释关键字的含义。页面关键词，就是那些本页面中比较重要的词，要想很好的了解本页面的内容，这些关键词对应的内容是非常重要的参考资料，在实现中，我们以页面内容中超链接的锚文字为基础，筛选那些链接指向的页面也包含本页关键字的链接锚文字，这一点是受到PageRank的思想所启发的；就效果来看，关键字筛选效果良好，但是这个条件明显太强了，在两个条件的限制下，大部分页面都只有7个以下的关键字，这样使得可视化的意义不大了，因为内容本来不多的情况下，单纯的文本就很容易理解的。参考文献，就是单纯是指维基百科词条页面中的参考文献，在目前的实现中，这些数据并没有加以利用。至于这些内容是如何从Web页面中提取的，主要靠对页面结构的分析，编写在网页爬虫的逻辑中。

第二步，是文本挖掘相关。针对上面的页面关键词，我们想要给每个页面关键词，根据相关度进行打分，然后根据这个相关度打分，进行进一步的可视化，比如比较重要的关键词Size比较大什么的。具体研究，使用了NLP中称为关键字提取（Keyword Extraction）的技术，通过阅读相关论文，确定了卡方相关性检验和仿PageRank的TextRank算法这两种技术方案。二选其一的话，我们选择了卡方检验，因为TextRank对我们来说有一些致命的缺点，比如计算量大、基于词在滑动窗口中的同时出现等等。相对来说，卡方检验对我们来说，要现实的多。

在我们参考的论文中，作者针对单篇论文，对关键字进行提取，打分自然就是卡方值。对语料数量的要求都不是特别高，但是对我们来说，我们现有的语料还是太少了，维基百科一般的词条页面中内容很少，用这些来进行关键词提取的话，估计效果甚微。于是我们决定扩充语料，经过研究，我们决定利用机器学习方面的会议(NIPS)及其提供的搜索引擎。因为项目做到这里我们已经决定先就一个词条，进行挖掘，实验下看下效果，所以就选了Machine_learning这个词条。扩充语料的过程还是用爬虫来完成的，模拟执行搜索引擎搜索，爬10篇论文下来作为语料。期间需要把pdf格式的转化成文本。经过一些折磨好歹完成语料的准备之后，开始进行关键词提取了，具体过程，一开始，我们想就第一步率先选出的关键字进行打分，算卡方值。但是语料准备好之后发现，我们选出的那些关键字，在这10篇论文中出现很少。究其原因，主要是我们扩充语料的思路有问题，试想维基百科中的词条，应该是比较宽泛的介绍性文字，而论文多是针对领域中某个小的问题的。从维基百科中挖掘出的关键词，很难应用到论文的语料中来。所以只能再次委曲求全，放弃对维基百科的关键字进行打分，改为直接从论文语料中进行关键词提取。

接下来的过程就是按照论文中的方法一步一步来了，涉及到词干提取，词组提取，词组聚类，计算卡方值等等。也顺利提取出了关键字，效果还不错。这么一来数据挖掘和准备的工作基本上就完成了。

第三步，可视化。可视化框架选择了D3js。用树状来表现分类，节点作为词条，点击词条，可视化展示此词条对应的关键词，使用了BubbleChart，主要涉及了D3js相关的javascript开发。

截止今天，以上的工作就是第一阶段做的事情。总的来说，没有成功的基于维基百科的数据进行挖掘，但是也探索了一条可行的路线，只是这条路线看样子扩展性并不好。于是，近几天忙于寻找一些更好的方式，不论是从数据源还是从挖掘的效果上。最近发现了一个专业的维基百科挖掘工具，(WikipediaMiner)[http://wikipedia-miner.cms.waikato.ac.nz/index.html]和weka出自同一所大学，[效果](http://sepans.com/wikistalker/)非常好。所以下一步，决定针对这个进行一些研究。先研究其原理，对比下我们挖掘的算法，看看差距在哪里，然后试图利用这个现有的工具，做一些工作。